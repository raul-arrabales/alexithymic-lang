{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Analysis of Alexithymic Discourse\n",
    "\n",
    "<hr>\n",
    "\n",
    "Alexithymic Language Project / raul@psicobotica.com / V2 release (sept 2020)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Sentiment Analysis Lexicons (Spanish)\n",
    "\n",
    "- Multilingual Sentiment Project (Spanish) [link](https://sites.google.com/site/datascienceslab/projects/multilingualsentiment).\n",
    "- AFINN-165 (English) [link](https://github.com/fnielsen/afinn). \n",
    "- AFINN-165-ES (my translation of AFINN-165). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Project (Spanish)\n",
    "Lists of positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_pos_words_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/Multilingualsentiment_positive_words_es.txt\"\n",
    "msp_neg_words_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/Multilingualsentiment_negative_words_es.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_text_file = open(msp_pos_words_path, \"r\")\n",
    "# neg_text_file = open(msp_neg_words_path, \"r\")\n",
    "# Load from URL instead of local disk\n",
    "pos_text_file =  urllib.request.urlopen(msp_pos_words_path)\n",
    "neg_text_file =  urllib.request.urlopen(msp_neg_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset = pos_text_file.info().get_content_charset()\n",
    "charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format is one word per line\n",
    "pos_lines = pos_text_file.read().decode(charset)\n",
    "neg_lines = neg_text_file.read().decode(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lines = pos_lines.splitlines()\n",
    "neg_lines = neg_lines.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720 1555\n"
     ]
    }
   ],
   "source": [
    "print(\"%2d %2d\" % (len(neg_lines), len(pos_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aire', 'restos', 'canal', 'pasado', 'falta', 'problema']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_lines[34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grande', 'realizar', 'firme', 'profesional', 'similar', 'libre']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lines[34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sets\n",
    "pos_words_set = set(pos_lines)\n",
    "neg_words_set = set(neg_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_words_set.intersection(neg_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4275"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_set = pos_words_set.union(neg_words_set)\n",
    "len(all_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_POS: 0; N_NEG: 1\n"
     ]
    }
   ],
   "source": [
    "test_phrase = \"Que asco de vida\".split()\n",
    "\n",
    "n_pos = np.sum([word in pos_words_set for word in test_phrase])\n",
    "n_neg = np.sum([word in neg_words_set for word in test_phrase])\n",
    "\n",
    "print(\"N_POS:%2d; N_NEG:%2d\" % (n_pos, n_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering stems instead of full words\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Porter stemmer\n",
    "p_stemmer = PorterStemmer() \n",
    "\n",
    "pos_stems = [p_stemmer.stem(word) for word in pos_lines]\n",
    "neg_stems = [p_stemmer.stem(word) for word in neg_lines]\n",
    "\n",
    "pos_stems_set = set(pos_stems)\n",
    "neg_stems_set = set(neg_stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_stems_set.intersection(pos_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_POS: 0; N_NEG: 1\n"
     ]
    }
   ],
   "source": [
    "test_phrase2 = \"Que asco de vida\".split()\n",
    "test_phrase2_s = [p_stemmer.stem(word) for word in test_phrase2]\n",
    "\n",
    "n_pos = np.sum([word in pos_stems_set for word in test_phrase2_s])\n",
    "n_neg = np.sum([word in neg_stems_set for word in test_phrase2_s])\n",
    "\n",
    "print(\"N_POS:%2d; N_NEG:%2d\" % (n_pos, n_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataframes\n",
    "pos_words_df = pd.DataFrame(list(pos_words_set), columns=['Pos'])\n",
    "neg_words_df = pd.DataFrame(list(neg_words_set), columns=['Neg'])\n",
    "pos_stems_df = pd.DataFrame(list(neg_stems_set), columns=['Pos'])\n",
    "neg_stems_df = pd.DataFrame(list(neg_stems_set), columns=['Neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word sets as df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Pos_Words.csv\"\n",
    "neg_words_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Neg_Words.csv\"\n",
    "pos_stems_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Pos_Stems.csv\"\n",
    "neg_stems_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Neg_Stems.csv\"\n",
    "\n",
    "pos_words_df.to_csv(pos_words_df_path, sep=';', encoding='utf-8', index=False)\n",
    "neg_words_df.to_csv(neg_words_df_path, sep=';', encoding='utf-8', index=False)\n",
    "pos_stems_df.to_csv(pos_stems_df_path, sep=';', encoding='utf-8', index=False)\n",
    "neg_stems_df.to_csv(neg_stems_df_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFINN-165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting AFINN sentiment analysis lexicon in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/AFINN-en-165.txt\"\n",
    "\n",
    "AFINN_df = pd.read_csv(AFINN_path, header=None, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_df.columns = ['Word', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Score\n",
       "0    abandon     -2\n",
       "1  abandoned     -2\n",
       "2   abandons     -2\n",
       "3   abducted     -2\n",
       "4  abduction     -2"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3382.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.617386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.124552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Score\n",
       "count  3382.000000\n",
       "mean     -0.617386\n",
       "std       2.124552\n",
       "min      -5.000000\n",
       "25%      -2.000000\n",
       "50%      -2.000000\n",
       "75%       2.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate FINN words into Spanish\n",
    "Using Google Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating with Google Translate API\n",
    "from googletrans import Translator \n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df = AFINN_df.copy()\n",
    "AFINN_es_df['Word_ES_Text'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celoso\n"
     ]
    }
   ],
   "source": [
    "# Apply in batches \n",
    "for i in range(3000,3382):\n",
    "    trlted = translator.translate(AFINN_df['Word'].iloc[i], src='en', dest='es').text\n",
    "    print(trlted)\n",
    "    AFINN_es_df['Word_ES_Text'].iloc[i] = trlted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "      <th>Word_ES</th>\n",
       "      <th>Word_ES_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>yucky</td>\n",
       "      <td>-2</td>\n",
       "      <td>Translated(src=en, dest=es, text=yucky, pronun...</td>\n",
       "      <td>yucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>yummy</td>\n",
       "      <td>3</td>\n",
       "      <td>Translated(src=en, dest=es, text=yummy, pronun...</td>\n",
       "      <td>sabroso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>zealot</td>\n",
       "      <td>-2</td>\n",
       "      <td>Translated(src=en, dest=es, text=zealot, pronu...</td>\n",
       "      <td>fanático</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>zealots</td>\n",
       "      <td>-2</td>\n",
       "      <td>Translated(src=en, dest=es, text=zealots, pron...</td>\n",
       "      <td>zealots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>zealous</td>\n",
       "      <td>2</td>\n",
       "      <td>Translated(src=en, dest=es, text=zealous, pron...</td>\n",
       "      <td>celoso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Score                                            Word_ES  \\\n",
       "3377    yucky     -2  Translated(src=en, dest=es, text=yucky, pronun...   \n",
       "3378    yummy      3  Translated(src=en, dest=es, text=yummy, pronun...   \n",
       "3379   zealot     -2  Translated(src=en, dest=es, text=zealot, pronu...   \n",
       "3380  zealots     -2  Translated(src=en, dest=es, text=zealots, pron...   \n",
       "3381  zealous      2  Translated(src=en, dest=es, text=zealous, pron...   \n",
       "\n",
       "     Word_ES_Text  \n",
       "3377        yucky  \n",
       "3378      sabroso  \n",
       "3379     fanático  \n",
       "3380      zealots  \n",
       "3381       celoso  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_es_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All at once (problems with Google API)\n",
    "# AFINN_df['Word_ES'] = AFINN_df.apply(\n",
    "#     lambda row: translator.translate(row.Word, src='en', dest='es').text, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (they appeared due to translation)\n",
    "AFINN_es_df = AFINN_es_df.drop_duplicates(subset='Word_ES_Text', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AFINN_df) - len(AFINN_es_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN_es_df = pd.read_csv(AFINN_es_path, header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\array\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# All to lower\n",
    "AFINN_es_df['Word_ES_Text_lower'] = AFINN_es_df.Word_ES_Text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df.drop('Word_ES', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df.drop('Word_ES_Text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df = AFINN_es_df.rename(columns={'Word_ES_Text_lower': 'Word_ES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "      <th>Word_ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandonado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestrado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Score      Word_ES\n",
       "0    abandon     -2    abandonar\n",
       "1  abandoned     -2   abandonado\n",
       "2   abandons     -2     abandona\n",
       "3   abducted     -2  secuestrado\n",
       "4  abduction     -2    secuestro"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_es_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Spanish Traslated AFINN df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\AFINN-165-es.csv\"\n",
    "AFINN_es_df.to_csv(AFINN_es_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
