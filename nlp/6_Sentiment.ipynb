{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Analysis of Alexithymic Discourse\n",
    "\n",
    "<hr>\n",
    "\n",
    "Alexithymic Language Project / raul@psicobotica.com / V2 release (sept 2020)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "We perform here different methods for sentiment analysis. Expressed sentiment variables might be use as part of the feature vectors for detecting alexithymia.\n",
    "\n",
    "- Lexicon-based sentiment analysis. \n",
    "- Third-party API based sentiment analysis. \n",
    "- Discussion and caveats about training our own sentiment analysis model. \n",
    "\n",
    "<hr>\n",
    "\n",
    "[More about Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features dataset\n",
    "- Data is already pre-processed (1-Preprocessing). \n",
    "- Basic NLP features are already calculated (2-Features). \n",
    "- Some additional BoW features have been added (3-BoW).\n",
    "- Some additional TF/IDF features have been added (3-TFIDF).\n",
    "- N-Gram models have been generated (3-N-Grams). \n",
    "- PoS lists for each document identified (4-Lexicosemantics). \n",
    "- Writer's personality variables inferred (5-Personality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dataset_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/data/Prolexitim_v2_features_5.csv\"\n",
    "alex_df = pd.read_csv(feats_dataset_path, header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>TAS20</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Card</th>\n",
       "      <th>T_Metaphors</th>\n",
       "      <th>T_ToM</th>\n",
       "      <th>...</th>\n",
       "      <th>consumption_preferences_music_playing</th>\n",
       "      <th>consumption_preferences_music_latin</th>\n",
       "      <th>consumption_preferences_music_rock</th>\n",
       "      <th>consumption_preferences_music_classical</th>\n",
       "      <th>consumption_preferences_read_frequency</th>\n",
       "      <th>consumption_preferences_books_entertainment_magazines</th>\n",
       "      <th>consumption_preferences_books_non_fiction</th>\n",
       "      <th>consumption_preferences_books_financial_investing</th>\n",
       "      <th>consumption_preferences_books_autobiographies</th>\n",
       "      <th>consumption_preferences_volunteer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1c9636c6a36ba79f847db0589528df65</td>\n",
       "      <td>66</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>9VH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Code  TAS20  F1  F2  F3  Gender  Age Card  \\\n",
       "361  1c9636c6a36ba79f847db0589528df65     66  26  19  21       2   18  9VH   \n",
       "2    20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22    1   \n",
       "\n",
       "     T_Metaphors  T_ToM  ...  consumption_preferences_music_playing  \\\n",
       "361            0      1  ...                                    0.0   \n",
       "2              0      1  ...                                    NaN   \n",
       "\n",
       "     consumption_preferences_music_latin  consumption_preferences_music_rock  \\\n",
       "361                                  0.0                                 0.5   \n",
       "2                                    NaN                                 NaN   \n",
       "\n",
       "     consumption_preferences_music_classical  \\\n",
       "361                                      0.0   \n",
       "2                                        NaN   \n",
       "\n",
       "    consumption_preferences_read_frequency  \\\n",
       "361                                    0.0   \n",
       "2                                      NaN   \n",
       "\n",
       "     consumption_preferences_books_entertainment_magazines  \\\n",
       "361                                                1.0       \n",
       "2                                                  NaN       \n",
       "\n",
       "     consumption_preferences_books_non_fiction  \\\n",
       "361                                        0.0   \n",
       "2                                          NaN   \n",
       "\n",
       "     consumption_preferences_books_financial_investing  \\\n",
       "361                                                0.0   \n",
       "2                                                  NaN   \n",
       "\n",
       "     consumption_preferences_books_autobiographies  \\\n",
       "361                                            0.0   \n",
       "2                                              NaN   \n",
       "\n",
       "    consumption_preferences_volunteer  \n",
       "361                               0.0  \n",
       "2                                 NaN  \n",
       "\n",
       "[2 rows x 175 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon-based sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment lexicon models\n",
    "- Previously generated (1b-SA-Lexicons). \n",
    "- We have lists of positives and negative words/stems obtained from Multilingual Sentiment Project. \n",
    "- We have my Spanish translation of AFINN-165 (AFINN-165-ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon files are already available at Github: \n",
    "path_MSP_Pos_Words = 'https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/MSP_Pos_Words.csv'\n",
    "path_MSP_Neg_Words = 'https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/MSP_Neg_Words.csv'\n",
    "path_MSP_Pos_StemsS = 'https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/MSP_Pos_StemsS.csv'\n",
    "path_MSP_Neg_StemsS = 'https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/MSP_Neg_StemsS.csv'\n",
    "path_AFINN_165_ES = 'https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/AFINN-165-es.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lexicons in memory as dataframes\n",
    "MSP_Pos_Words_df = pd.read_csv(path_MSP_Pos_Words, header=0, delimiter=\";\")\n",
    "MSP_Neg_Words_df = pd.read_csv(path_MSP_Neg_Words, header=0, delimiter=\";\")\n",
    "MSP_Pos_StemsS_df = pd.read_csv(path_MSP_Pos_StemsS, header=0, delimiter=\";\")\n",
    "MSP_Neg_StemsS_df = pd.read_csv(path_MSP_Neg_StemsS, header=0, delimiter=\";\")\n",
    "AFINN_165_ES_df = pd.read_csv(path_AFINN_165_ES, header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "      <th>Word_ES</th>\n",
       "      <th>Stem_ES_P</th>\n",
       "      <th>Stem_ES_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>crazier</td>\n",
       "      <td>-2</td>\n",
       "      <td>más loco</td>\n",
       "      <td>más loco</td>\n",
       "      <td>mas loc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>threaten</td>\n",
       "      <td>-2</td>\n",
       "      <td>threaten</td>\n",
       "      <td>threaten</td>\n",
       "      <td>threat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>vested</td>\n",
       "      <td>1</td>\n",
       "      <td>establecido</td>\n",
       "      <td>establecido</td>\n",
       "      <td>establec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>rebel</td>\n",
       "      <td>-2</td>\n",
       "      <td>rebelde</td>\n",
       "      <td>rebeld</td>\n",
       "      <td>rebeld</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Score      Word_ES    Stem_ES_P Stem_ES_S\n",
       "586    crazier     -2     más loco     más loco   mas loc\n",
       "2592  threaten     -2     threaten     threaten    threat\n",
       "2748    vested      1  establecido  establecido  establec\n",
       "2148     rebel     -2      rebelde       rebeld    rebeld"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_165_ES_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sets in the case of MSP, where there is no specific score\n",
    "MSP_Pos_Words_set = set(MSP_Pos_Words_df.Pos)\n",
    "MSP_Neg_Words_set = set(MSP_Neg_Words_df.Neg)\n",
    "MSP_Pos_StemsS_set = set(MSP_Pos_StemsS_df.Pos)\n",
    "MSP_Neg_StemsS_set = set(MSP_Neg_StemsS_df.Neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accesible', 'prolífico', 'estabilizar', 'estelar']\n",
      "['adherente', 'cuestiones', 'descarada', 'imprecisiones']\n",
      "['subvencion', 'aug', 'prefir', 'lind']\n",
      "['invis', 'villan', 'devor', 'deprim']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.sample(MSP_Pos_Words_set, 4))\n",
    "print(random.sample(MSP_Neg_Words_set, 4))\n",
    "print(random.sample(MSP_Pos_StemsS_set, 4))\n",
    "print(random.sample(MSP_Neg_StemsS_set, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sentimen per document\n",
    "- Based on MSP word matches. \n",
    "- Based on MSP snowball stems matches. \n",
    "- Based on AFINN word matches. \n",
    "- Based on AFINN snowball stems matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the sentiment score based on MSP pos/neg words/stems and doc length\n",
    "def get_MSP_Sent(doc, posWordsSet, negWordsSet):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : list\n",
    "        List of stopped tokens / snowballed stems extracted from a text. \n",
    "    posWordsSet : set\n",
    "        Set of positive words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "        (Positive matches - negative matches) / length of token list\n",
    "        \n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    for tok in doc: \n",
    "        if (tok in posWordsSet):\n",
    "            score += 1\n",
    "        if (tok in negWordsSet):\n",
    "            score -= 1\n",
    "            \n",
    "    return (score / len(doc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code', 'TAS20', 'F1', 'F2', 'F3', 'Gender', 'Age', 'Card',\n",
       "       'T_Metaphors', 'T_ToM', 'T_FP', 'T_Interpret', 'T_Desc', 'T_Confussion',\n",
       "       'Text', 'Alex_A', 'Alex_B', 'Words', 'Sentences', 'Tokens',\n",
       "       'Tokens_Stop', 'Tokens_Stem_P', 'Tokens_Stem_S', 'POS', 'NER', 'DEP',\n",
       "       'Lemmas_CNLP', 'Lemmas_Spacy', 'Chars', 'avgWL', 'avgSL', 'Pun_Count',\n",
       "       'Stop_Count', 'RawTokens', 'Title_Count', 'Upper_Count', 'PRON_Count',\n",
       "       'DET_Count', 'ADV_Count', 'VERB_Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.columns[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niño', 'tras', 'horas', 'práctica', 'infructuosas', 'violín', 'punto', 'abandonar', 'estudios', 'lugar', 'ello', 'tras', 'dejarlo', 'reposar', 'mesa', 'explorar', 'estructura', 'materiales', 'funcionamiento', 'supo', 'encontrar', 'transformar', 'frustración', 'camino', 'aprendizaje', 'interactivo', 'instrumento', 'compañero', 'nuevas', 'aventuras', 'llenas', 'asombro']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.03125"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with words\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stop'][159])\n",
    "print(test_tokens)\n",
    "get_MSP_Sent(test_tokens, MSP_Pos_Words_set, MSP_Neg_Words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niñ', 'tras', 'hor', 'practic', 'infructu', 'violin', 'punt', 'abandon', 'estudi', 'lug', 'ello', 'tras', 'dej', 'rep', 'mes', 'explor', 'estructur', 'material', 'funcion', 'sup', 'encontr', 'transform', 'frustracion', 'camin', 'aprendizaj', 'interact', 'instrument', 'compañer', 'nuev', 'aventur', 'llen', 'asombr']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.03125"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with stems\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stem_S'][159])\n",
    "print(test_tokens)\n",
    "get_MSP_Sent(test_tokens, MSP_Pos_StemsS_set, MSP_Neg_StemsS_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, add the MSP sentiment score to all examplars\n",
    "alex_df['MSP_Words'] = alex_df.Tokens_Stop.apply(lambda x: get_MSP_Sent(ast.literal_eval(x), MSP_Pos_Words_set, MSP_Neg_Words_set))\n",
    "alex_df['MSP_Stems'] = alex_df.Tokens_Stem_S.apply(lambda x: get_MSP_Sent(ast.literal_eval(x), MSP_Pos_StemsS_set, MSP_Neg_StemsS_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSP_Words</th>\n",
       "      <th>MSP_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381.000000</td>\n",
       "      <td>381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.034257</td>\n",
       "      <td>-0.072769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.201612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSP_Words   MSP_Stems\n",
       "count  381.000000  381.000000\n",
       "mean    -0.034257   -0.072769\n",
       "std      0.135139    0.201612\n",
       "min     -0.600000   -0.800000\n",
       "25%     -0.100000   -0.200000\n",
       "50%      0.000000   -0.058824\n",
       "75%      0.040000    0.057143\n",
       "max      0.300000    0.500000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['MSP_Words','MSP_Stems']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do AFINN now: \n",
    "- Polarity: (positive scores - negative scores).\n",
    "- Mean Intensity: mean(sum(positives),sum(abs(negatives))). \n",
    "- Max Intensity: max(sum(positives),sum(abs(negatives)))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the sentiment score (polarity) based on AFINN-ES score\n",
    "def get_AFINN_Polarity(doc, isStem, AFINN):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : list\n",
    "        List of stopped tokens / snowballed stems extracted from a text. \n",
    "    isStem: boolean\n",
    "        True means stem, false indicates word. \n",
    "    AFINN : dataframe\n",
    "        Dataframe with words, stems and their corresponding sentiment score.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: int\n",
    "        sum(score(Positive matches)) - sum(score(negative matches)))\n",
    "        \n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    if (isStem):\n",
    "        for tok in doc: \n",
    "            points = AFINN[AFINN['Stem_ES_S'] == tok]['Score']\n",
    "            if (len(points) == 1):\n",
    "                # print(\"Word: \" + tok + \" -> \" + str(points.item()))\n",
    "                score += points.item()\n",
    "    else: \n",
    "        for tok in doc: \n",
    "            points = AFINN[AFINN['Word_ES'] == tok]['Score']\n",
    "            if (len(points) == 1):\n",
    "                # print(\"Word: \" + tok + \" -> \" + str(points.item()))\n",
    "                score += points.item()\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niño', 'obligaban', 'tocar', 'violín', 'gustaba', 'música', 'bueno', 'tocando', 'instrumentos', 'padres', 'obligaban', 'ir', 'conservatorio', 'tomar', 'clases', 'música', 'daba', 'vergüenza', 'hacerlo', 'delante', 'amigos', 'llegar', 'casa', 'estudiar', 'partituras', 'aburría', 'agobiaba', 'vez']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with words\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stop'][69])\n",
    "print(test_tokens)\n",
    "get_AFINN_Polarity(test_tokens, False, AFINN_165_ES_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niñ', 'oblig', 'toc', 'violin', 'gust', 'music', 'buen', 'toc', 'instrument', 'padr', 'oblig', 'ir', 'conservatori', 'tom', 'clas', 'music', 'dab', 'vergüenz', 'hac', 'delant', 'amig', 'lleg', 'cas', 'estudi', 'partitur', 'aburr', 'agobi', 'vez']\n",
      "Word: oblig -> 1\n",
      "Word: buen -> 3\n",
      "Word: oblig -> 1\n",
      "Word: vergüenz -> -2\n",
      "Word: amig -> 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with stems\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stem_S'][69])\n",
    "print(test_tokens)\n",
    "get_AFINN_Polarity(test_tokens, True, AFINN_165_ES_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the sentiment mean intensity based on AFINN-ES score\n",
    "def get_AFINN_Intensity(doc, isStem, AFINN):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : list\n",
    "        List of stopped tokens / snowballed stems extracted from a text. \n",
    "    isStem: boolean\n",
    "        True means stem, false indicates word. \n",
    "    AFINN : dataframe\n",
    "        Dataframe with words, stems and their corresponding sentiment score.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "         mean(sum(positives),sum(abs(negatives))).\n",
    "        \n",
    "    \"\"\"\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    \n",
    "    if (isStem):\n",
    "        for tok in doc: \n",
    "            points = AFINN[AFINN['Stem_ES_S'] == tok]['Score']\n",
    "            if (len(points) == 1):\n",
    "                score = points.item()\n",
    "                # print(\"Word: \" + tok + \" -> \" + str(score))\n",
    "                if (score > 0):\n",
    "                    positives += score\n",
    "                else: \n",
    "                    negatives += abs(score)\n",
    "    else: \n",
    "        for tok in doc: \n",
    "            points = AFINN[AFINN['Word_ES'] == tok]['Score']\n",
    "            if (len(points) == 1):\n",
    "                score = points.item()\n",
    "                # print(\"Word: \" + tok + \" -> \" + str(score))\n",
    "                if (score > 0):\n",
    "                    positives += score\n",
    "                else: \n",
    "                    negatives += abs(score)\n",
    "                            \n",
    "    return ((positives+negatives)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niño', 'obligaban', 'tocar', 'violín', 'gustaba', 'música', 'bueno', 'tocando', 'instrumentos', 'padres', 'obligaban', 'ir', 'conservatorio', 'tomar', 'clases', 'música', 'daba', 'vergüenza', 'hacerlo', 'delante', 'amigos', 'llegar', 'casa', 'estudiar', 'partituras', 'aburría', 'agobiaba', 'vez']\n",
      "Word: bueno -> 3\n",
      "Word: vergüenza -> -2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with words\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stop'][69])\n",
    "print(test_tokens)\n",
    "get_AFINN_Intensity(test_tokens, False, AFINN_165_ES_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niñ', 'oblig', 'toc', 'violin', 'gust', 'music', 'buen', 'toc', 'instrument', 'padr', 'oblig', 'ir', 'conservatori', 'tom', 'clas', 'music', 'dab', 'vergüenz', 'hac', 'delant', 'amig', 'lleg', 'cas', 'estudi', 'partitur', 'aburr', 'agobi', 'vez']\n",
      "Word: oblig -> 1\n",
      "Word: buen -> 3\n",
      "Word: oblig -> 1\n",
      "Word: vergüenz -> -2\n",
      "Word: amig -> 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with stems\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stem_S'][69])\n",
    "print(test_tokens)\n",
    "get_AFINN_Intensity(test_tokens, True, AFINN_165_ES_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the sentiment maximum intensity based on AFINN-ES score\n",
    "def get_AFINN_Max_Intensity(doc, isStem, AFINN):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : list\n",
    "        List of stopped tokens / snowballed stems extracted from a text. \n",
    "    isStem: boolean\n",
    "        True means stem, false indicates word. \n",
    "    AFINN : dataframe\n",
    "        Dataframe with words, stems and their corresponding sentiment score.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "         max(sum(positives),sum(abs(negatives))).\n",
    "        \n",
    "    \"\"\"\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    \n",
    "    if (isStem):\n",
    "        for tok in doc: \n",
    "            points = AFINN[AFINN['Stem_ES_S'] == tok]['Score']\n",
    "            if (len(points) == 1):\n",
    "                score = points.item()\n",
    "                # print(\"Word: \" + tok + \" -> \" + str(score))\n",
    "                if (score > 0):\n",
    "                    positives += score\n",
    "                else: \n",
    "                    negatives += abs(score)\n",
    "    else: \n",
    "        for tok in doc: \n",
    "            points = AFINN[AFINN['Word_ES'] == tok]['Score']\n",
    "            if (len(points) == 1):\n",
    "                score = points.item()\n",
    "                # print(\"Word: \" + tok + \" -> \" + str(score))\n",
    "                if (score > 0):\n",
    "                    positives += score\n",
    "                else: \n",
    "                    negatives += abs(score)\n",
    "                            \n",
    "    if (positives >= negatives):\n",
    "        return positives\n",
    "    else:\n",
    "        return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niño', 'obligaban', 'tocar', 'violín', 'gustaba', 'música', 'bueno', 'tocando', 'instrumentos', 'padres', 'obligaban', 'ir', 'conservatorio', 'tomar', 'clases', 'música', 'daba', 'vergüenza', 'hacerlo', 'delante', 'amigos', 'llegar', 'casa', 'estudiar', 'partituras', 'aburría', 'agobiaba', 'vez']\n",
      "Word: bueno -> 3\n",
      "Word: vergüenza -> -2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with words\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stop'][69])\n",
    "print(test_tokens)\n",
    "get_AFINN_Max_Intensity(test_tokens, False, AFINN_165_ES_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['niñ', 'oblig', 'toc', 'violin', 'gust', 'music', 'buen', 'toc', 'instrument', 'padr', 'oblig', 'ir', 'conservatori', 'tom', 'clas', 'music', 'dab', 'vergüenz', 'hac', 'delant', 'amig', 'lleg', 'cas', 'estudi', 'partitur', 'aburr', 'agobi', 'vez']\n",
      "Word: oblig -> 1\n",
      "Word: buen -> 3\n",
      "Word: oblig -> 1\n",
      "Word: vergüenz -> -2\n",
      "Word: amig -> 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function with stems\n",
    "test_tokens = ast.literal_eval(alex_df['Tokens_Stem_S'][69])\n",
    "print(test_tokens)\n",
    "get_AFINN_Max_Intensity(test_tokens, True, AFINN_165_ES_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, add the AFINN sentiment scores to all exemplars\n",
    "alex_df['AFINN_Words_Pol'] = alex_df.Tokens_Stop.apply(lambda x: get_AFINN_Polarity(ast.literal_eval(x), False, AFINN_165_ES_df))\n",
    "alex_df['AFINN_Stems_Pol'] = alex_df.Tokens_Stem_S.apply(lambda x: get_AFINN_Polarity(ast.literal_eval(x), True, AFINN_165_ES_df))\n",
    "alex_df['AFINN_Words_Int'] = alex_df.Tokens_Stop.apply(lambda x: get_AFINN_Intensity(ast.literal_eval(x), False, AFINN_165_ES_df))\n",
    "alex_df['AFINN_Stems_Int'] = alex_df.Tokens_Stem_S.apply(lambda x: get_AFINN_Intensity(ast.literal_eval(x), True, AFINN_165_ES_df))\n",
    "alex_df['AFINN_Words_Max'] = alex_df.Tokens_Stop.apply(lambda x: get_AFINN_Max_Intensity(ast.literal_eval(x), False, AFINN_165_ES_df))\n",
    "alex_df['AFINN_Stems_Max'] = alex_df.Tokens_Stem_S.apply(lambda x: get_AFINN_Max_Intensity(ast.literal_eval(x), True, AFINN_165_ES_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFINN_Words_Pol</th>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "      <th>AFINN_Words_Int</th>\n",
       "      <th>AFINN_Stems_Int</th>\n",
       "      <th>AFINN_Words_Max</th>\n",
       "      <th>AFINN_Stems_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>381.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.456693</td>\n",
       "      <td>-0.314961</td>\n",
       "      <td>1.482940</td>\n",
       "      <td>1.748031</td>\n",
       "      <td>2.406824</td>\n",
       "      <td>2.716535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.854606</td>\n",
       "      <td>2.892915</td>\n",
       "      <td>1.752546</td>\n",
       "      <td>1.806648</td>\n",
       "      <td>2.684234</td>\n",
       "      <td>2.667241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-11.000000</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AFINN_Words_Pol  AFINN_Stems_Pol  AFINN_Words_Int  AFINN_Stems_Int  \\\n",
       "count       381.000000       381.000000       381.000000       381.000000   \n",
       "mean         -0.456693        -0.314961         1.482940         1.748031   \n",
       "std           2.854606         2.892915         1.752546         1.806648   \n",
       "min         -11.000000       -11.000000         0.000000         0.000000   \n",
       "25%          -2.000000        -2.000000         0.000000         0.000000   \n",
       "50%           0.000000         0.000000         1.000000         1.500000   \n",
       "75%           0.000000         1.000000         2.000000         2.500000   \n",
       "max          11.000000        11.000000        10.000000         9.000000   \n",
       "\n",
       "       AFINN_Words_Max  AFINN_Stems_Max  \n",
       "count       381.000000       381.000000  \n",
       "mean          2.406824         2.716535  \n",
       "std           2.684234         2.667241  \n",
       "min           0.000000         0.000000  \n",
       "25%           0.000000         0.000000  \n",
       "50%           2.000000         2.000000  \n",
       "75%           4.000000         4.000000  \n",
       "max          15.000000        14.000000  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['AFINN_Words_Pol',\n",
    "         'AFINN_Stems_Pol','AFINN_Words_Int','AFINN_Stems_Int',\n",
    "         'AFINN_Words_Max','AFINN_Stems_Max']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFINN_Words_Pol</th>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "      <th>AFINN_Words_Int</th>\n",
       "      <th>AFINN_Stems_Int</th>\n",
       "      <th>AFINN_Words_Max</th>\n",
       "      <th>AFINN_Stems_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AFINN_Words_Pol</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457984</td>\n",
       "      <td>-0.064684</td>\n",
       "      <td>0.052638</td>\n",
       "      <td>-0.117186</td>\n",
       "      <td>0.045166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "      <td>0.457984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>-0.020763</td>\n",
       "      <td>-0.009550</td>\n",
       "      <td>-0.037180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Words_Int</th>\n",
       "      <td>-0.064684</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.620746</td>\n",
       "      <td>0.961139</td>\n",
       "      <td>0.601903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Stems_Int</th>\n",
       "      <td>0.052638</td>\n",
       "      <td>-0.020763</td>\n",
       "      <td>0.620746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570630</td>\n",
       "      <td>0.954758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Words_Max</th>\n",
       "      <td>-0.117186</td>\n",
       "      <td>-0.009550</td>\n",
       "      <td>0.961139</td>\n",
       "      <td>0.570630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Stems_Max</th>\n",
       "      <td>0.045166</td>\n",
       "      <td>-0.037180</td>\n",
       "      <td>0.601903</td>\n",
       "      <td>0.954758</td>\n",
       "      <td>0.572275</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AFINN_Words_Pol  AFINN_Stems_Pol  AFINN_Words_Int  \\\n",
       "AFINN_Words_Pol         1.000000         0.457984        -0.064684   \n",
       "AFINN_Stems_Pol         0.457984         1.000000         0.001014   \n",
       "AFINN_Words_Int        -0.064684         0.001014         1.000000   \n",
       "AFINN_Stems_Int         0.052638        -0.020763         0.620746   \n",
       "AFINN_Words_Max        -0.117186        -0.009550         0.961139   \n",
       "AFINN_Stems_Max         0.045166        -0.037180         0.601903   \n",
       "\n",
       "                 AFINN_Stems_Int  AFINN_Words_Max  AFINN_Stems_Max  \n",
       "AFINN_Words_Pol         0.052638        -0.117186         0.045166  \n",
       "AFINN_Stems_Pol        -0.020763        -0.009550        -0.037180  \n",
       "AFINN_Words_Int         0.620746         0.961139         0.601903  \n",
       "AFINN_Stems_Int         1.000000         0.570630         0.954758  \n",
       "AFINN_Words_Max         0.570630         1.000000         0.572275  \n",
       "AFINN_Stems_Max         0.954758         0.572275         1.000000  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['AFINN_Words_Pol',\n",
    "         'AFINN_Stems_Pol','AFINN_Words_Int','AFINN_Stems_Int',\n",
    "         'AFINN_Words_Max','AFINN_Stems_Max']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSP_Stems</th>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSP_Stems</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.247254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "      <td>0.247254</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MSP_Stems  AFINN_Stems_Pol\n",
       "MSP_Stems         1.000000         0.247254\n",
       "AFINN_Stems_Pol   0.247254         1.000000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['MSP_Stems','AFINN_Stems_Pol']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSP_Words', 'MSP_Stems', 'AFINN_Words_Pol', 'AFINN_Stems_Pol',\n",
       "       'AFINN_Words_Int', 'AFINN_Stems_Int', 'AFINN_Words_Max',\n",
       "       'AFINN_Stems_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 8 new features related to sentiment. \n",
    "alex_df.columns[len(alex_df.columns)-8:len(alex_df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using IBM Cloud\n",
    "- Watson Natural Language Understanding (NLU) API\n",
    "\n",
    "[Watson NLU](https://www.ibm.com/es-es/cloud/watson-natural-language-understanding)\n",
    "\n",
    "[API Guide - Python](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python)\n",
    "\n",
    "[Limited support for Spanish Language](https://cloud.ibm.com/docs/natural-language-understanding?topic=natural-language-understanding-language-support#spanish)\n",
    "\n",
    "- Basically, Watson NLU provides a sentiment score for Spanish, but not the details about different emotions (for that, we'd need to translate first into English). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, EmotionOptions, SentimentOptions\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import ApiException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watson NLU endpoint for Europe\n",
    "NLU_URL = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key stored in local file apikey.json\n",
    "with open('apikey.json') as f:\n",
    "    apikeydata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apikeydata.get('NLU_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator(apikeydata.get('NLU_key'))\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2020-08-01',\n",
    "    authenticator=authenticator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_understanding.set_service_url(NLU_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the API\n",
    "# Making the call for Emotion and Sentiment\n",
    "response = natural_language_understanding.analyze(\n",
    "    text='Probamos con una frase en español, aunque IBM dice que el idioma español no está soportado para las funciones de emociones, pero sí para las de análisis del sentimiento.',\n",
    "    features=Features(emotion=EmotionOptions(),\n",
    "                      sentiment=SentimentOptions())).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the API\n",
    "# Making the call for Emotion and Sentiment\n",
    "response = natural_language_understanding.analyze(\n",
    "    text='dos palabras más',\n",
    "    features=Features(sentiment=SentimentOptions())).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"usage\": {\n",
      "    \"text_units\": 1,\n",
      "    \"text_characters\": 16,\n",
      "    \"features\": 1\n",
      "  },\n",
      "  \"sentiment\": {\n",
      "    \"document\": {\n",
      "      \"score\": 0,\n",
      "      \"label\": \"neutral\"\n",
      "    }\n",
      "  },\n",
      "  \"language\": \"es\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check the result:\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['sentiment']['document']['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's annotate our data with the sentiment score from IBM NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calls the Watson NLU API and gets the sentiment score\n",
    "# for a given plain text in Spanish.\n",
    "def get_Watson_Sentiment(text_es):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_es : str\n",
    "        Document to be analyzed in Spanish\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    score: float\n",
    "        Document sentiment analysis results (sentiment score)\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_response = natural_language_understanding.analyze(\n",
    "            text = text_es,\n",
    "            features = Features(sentiment=SentimentOptions())).get_result()\n",
    "        \n",
    "    except ApiException as ex:\n",
    "        return np.nan\n",
    "        print(\"Method failed with status code \" + str(ex.code) + \": \" + ex.message)\n",
    "        \n",
    "    return json_response['sentiment']['document']['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply for all rows: \n",
    "# alex_df['Watson_Sent'] = alex_df.Text.apply(lambda x: get_Watson_Sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty column\n",
    "alex_df['Watson_Sent'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) Watson said: -0.621737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\array\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Watson said: -0.946776\n",
      "(2) Watson said: -0.809901\n",
      "(3) Watson said: -0.964077\n",
      "(4) Watson said: -0.599994\n",
      "(5) Watson said: -0.8963\n",
      "(6) Watson said: 0\n",
      "(7) Watson said: 0.720764\n",
      "(8) Watson said: -0.733411\n",
      "(9) Watson said: 0.315651\n",
      "(10) Watson said: 0.630662\n",
      "(11) Watson said: -0.405908\n",
      "(12) Watson said: -0.861215\n",
      "(13) Watson said: 0\n",
      "(14) Watson said: -0.5975\n",
      "(15) Watson said: 0.424281\n",
      "(16) Watson said: -0.891803\n",
      "(17) Watson said: -0.531513\n",
      "(18) Watson said: 0.450271\n",
      "(19) Watson said: -0.326503\n",
      "(20) Watson said: 0\n",
      "(21) Watson said: -0.429521\n",
      "(22) Watson said: -0.701011\n",
      "(23) Watson said: -0.855875\n",
      "(24) Watson said: 0\n",
      "(25) Watson said: 0\n",
      "(26) Watson said: -0.592164\n",
      "(27) Watson said: 0\n",
      "(28) Watson said: 0\n",
      "(29) Watson said: 0\n",
      "(30) Watson said: -0.70638\n",
      "(31) Watson said: -0.709653\n",
      "(32) Watson said: -0.814213\n",
      "(33) Watson said: 0.93461\n",
      "(34) Watson said: 0\n",
      "(35) Watson said: -0.866115\n",
      "(36) Watson said: 0\n",
      "(37) Watson said: 0.673749\n",
      "(38) Watson said: 0\n",
      "(39) Watson said: 0.457191\n",
      "(40) Watson said: -0.764797\n",
      "(41) Watson said: 0.61661\n",
      "(42) Watson said: 0.751663\n",
      "(43) Watson said: 0\n",
      "(44) Watson said: -0.835073\n",
      "(45) Watson said: -0.713724\n",
      "(46) Watson said: 0.322757\n",
      "(47) Watson said: 0.837094\n",
      "(48) Watson said: -0.656065\n",
      "(49) Watson said: -0.360206\n",
      "(50) Watson said: 0\n",
      "(51) Watson said: 0.756302\n",
      "(52) Watson said: 0\n",
      "(53) Watson said: -0.484692\n",
      "(54) Watson said: -0.477622\n",
      "(55) Watson said: 0\n",
      "(56) Watson said: 0\n",
      "(57) Watson said: 0\n",
      "(58) Watson said: -0.690675\n",
      "(59) Watson said: 0.80759\n",
      "(60) Watson said: -0.724648\n",
      "(61) Watson said: 0\n",
      "(62) Watson said: -0.887249\n",
      "(63) Watson said: 0.525301\n",
      "(64) Watson said: -0.940156\n",
      "(65) Watson said: -0.685697\n",
      "(66) Watson said: 0.845514\n",
      "(67) Watson said: 0.78518\n",
      "(68) Watson said: 0.54055\n",
      "(69) Watson said: -0.364884\n",
      "(70) Watson said: 0.494877\n",
      "(71) Watson said: 0\n",
      "(72) Watson said: 0.658341\n",
      "(73) Watson said: -0.956248\n",
      "(74) Watson said: -0.895893\n",
      "(75) Watson said: 0\n",
      "(76) Watson said: -0.85348\n",
      "(77) Watson said: -0.871748\n",
      "(78) Watson said: 0\n",
      "(79) Watson said: 0\n",
      "(80) Watson said: -0.273984\n",
      "(81) Watson said: -0.902752\n",
      "(82) Watson said: 0\n",
      "(83) Watson said: -0.658213\n",
      "(84) Watson said: 0.919864\n",
      "(85) Watson said: 0.927837\n",
      "(86) Watson said: 0.679537\n",
      "(87) Watson said: -0.651548\n",
      "(88) Watson said: 0\n",
      "(89) Watson said: -0.259907\n",
      "(90) Watson said: -0.367525\n",
      "(91) Watson said: 0\n",
      "(92) Watson said: 0\n",
      "(93) Watson said: 0\n",
      "(94) Watson said: 0\n",
      "(95) Watson said: 0\n",
      "(96) Watson said: 0\n",
      "(97) Watson said: 0\n",
      "(98) Watson said: -0.615658\n",
      "(99) Watson said: 0.952195\n",
      "(100) Watson said: 0.30823\n",
      "(101) Watson said: -0.921301\n",
      "(102) Watson said: -0.805514\n",
      "(103) Watson said: 0.702331\n",
      "(104) Watson said: 0\n",
      "(105) Watson said: 0\n",
      "(106) Watson said: -0.814973\n",
      "(107) Watson said: 0.372589\n",
      "(108) Watson said: 0.991864\n",
      "(109) Watson said: -0.895626\n",
      "(110) Watson said: 0.35375\n",
      "(111) Watson said: -0.513289\n",
      "(112) Watson said: 0.772403\n",
      "(113) Watson said: 0.534224\n",
      "(114) Watson said: 0.567231\n",
      "(115) Watson said: -0.976379\n",
      "(116) Watson said: -0.367296\n",
      "(117) Watson said: 0.735812\n",
      "(118) Watson said: 0.593823\n",
      "(119) Watson said: 0\n",
      "(120) Watson said: 0.581042\n",
      "(121) Watson said: 0.540604\n",
      "(122) Watson said: -0.790846\n",
      "(123) Watson said: 0.371627\n",
      "(124) Watson said: 0\n",
      "(125) Watson said: 0\n",
      "(126) Watson said: 0.86542\n",
      "(127) Watson said: -0.582711\n",
      "(128) Watson said: -0.373696\n",
      "(129) Watson said: -0.899936\n",
      "(130) Watson said: -0.579653\n",
      "(131) Watson said: 0.799855\n",
      "(132) Watson said: -0.585205\n",
      "(133) Watson said: -0.676965\n",
      "(134) Watson said: -0.443237\n",
      "(135) Watson said: 0\n",
      "(136) Watson said: -0.887842\n",
      "(137) Watson said: 0.468665\n",
      "(138) Watson said: -0.837468\n",
      "(139) Watson said: 0\n",
      "(140) Watson said: -0.965078\n",
      "(141) Watson said: -0.725956\n",
      "(142) Watson said: 0.393457\n",
      "(143) Watson said: 0.368025\n",
      "(144) Watson said: -0.637103\n",
      "(145) Watson said: 0\n",
      "(146) Watson said: -0.588189\n",
      "(147) Watson said: -0.508307\n",
      "(148) Watson said: 0.684852\n",
      "(149) Watson said: 0.652914\n",
      "(150) Watson said: 0.694665\n",
      "(151) Watson said: 0\n",
      "(152) Watson said: 0.839815\n",
      "(153) Watson said: -0.356843\n",
      "(154) Watson said: 0.797364\n",
      "(155) Watson said: 0.730535\n",
      "(156) Watson said: 0.60059\n",
      "(157) Watson said: 0\n",
      "(158) Watson said: -0.48511\n",
      "(159) Watson said: 0\n",
      "(160) Watson said: 0\n",
      "(161) Watson said: 0.403927\n",
      "(162) Watson said: -0.413254\n",
      "(163) Watson said: -0.740396\n",
      "(164) Watson said: -0.422817\n",
      "(165) Watson said: -0.57954\n",
      "(166) Watson said: -0.703882\n",
      "(167) Watson said: -0.557228\n",
      "(168) Watson said: -0.582201\n",
      "(169) Watson said: 0.427424\n",
      "(170) Watson said: -0.52729\n",
      "(171) Watson said: -0.494523\n",
      "(172) Watson said: -0.610292\n",
      "(173) Watson said: -0.640675\n",
      "(174) Watson said: -0.579004\n",
      "(175) Watson said: -0.70728\n",
      "(176) Watson said: 0.831919\n",
      "(177) Watson said: 0\n",
      "(178) Watson said: 0\n",
      "(179) Watson said: -0.963024\n",
      "(180) Watson said: -0.414596\n",
      "(181) Watson said: 0.777032\n",
      "(182) Watson said: -0.623549\n",
      "(183) Watson said: -0.539328\n",
      "(184) Watson said: 0\n",
      "(185) Watson said: -0.995101\n",
      "(186) Watson said: 0.561113\n",
      "(187) Watson said: -0.861066\n",
      "(188) Watson said: 0\n",
      "(189) Watson said: 0\n",
      "(190) Watson said: -0.510835\n",
      "(191) Watson said: 0.814494\n",
      "(192) Watson said: 0.777306\n",
      "(193) Watson said: 0\n",
      "(194) Watson said: 0.675274\n",
      "(195) Watson said: 0\n",
      "(196) Watson said: 0.665675\n",
      "(197) Watson said: 0\n",
      "(198) Watson said: 0\n",
      "(199) Watson said: -0.506357\n",
      "(200) Watson said: -0.717433\n",
      "(201) Watson said: 0.755585\n",
      "(202) Watson said: 0.948395\n",
      "(203) Watson said: 0.542283\n",
      "(204) Watson said: 0\n",
      "(205) Watson said: 0\n",
      "(206) Watson said: -0.414574\n",
      "(207) Watson said: -0.807008\n",
      "(208) Watson said: -0.75032\n",
      "(209) Watson said: 0.621343\n",
      "(210) Watson said: -0.573674\n",
      "(211) Watson said: -0.467887\n",
      "(212) Watson said: 0.842555\n",
      "(213) Watson said: 0\n",
      "(214) Watson said: -0.974847\n",
      "(215) Watson said: 0.840036\n",
      "(216) Watson said: 0.759027\n",
      "(217) Watson said: 0.372199\n",
      "(218) Watson said: -0.585809\n",
      "(219) Watson said: 0.937084\n",
      "(220) Watson said: -0.55287\n",
      "(221) Watson said: 0.692803\n",
      "(222) Watson said: 0\n",
      "(223) Watson said: 0.712702\n",
      "(224) Watson said: 0.55504\n",
      "(225) Watson said: -0.533346\n",
      "(226) Watson said: -0.824255\n",
      "(227) Watson said: -0.500331\n",
      "(228) Watson said: 0.974335\n",
      "(229) Watson said: 0\n",
      "(230) Watson said: 0.667139\n",
      "(231) Watson said: 0.872396\n",
      "(232) Watson said: -0.387557\n",
      "(233) Watson said: 0.884209\n",
      "(234) Watson said: -0.434432\n",
      "(235) Watson said: 0.717381\n",
      "(236) Watson said: 0\n",
      "(237) Watson said: 0.858014\n",
      "(238) Watson said: -0.646515\n",
      "(239) Watson said: 0\n",
      "(240) Watson said: 0\n",
      "(241) Watson said: 0\n",
      "(242) Watson said: -0.6974\n",
      "(243) Watson said: 0.689151\n",
      "(244) Watson said: 0.530351\n",
      "(245) Watson said: 0\n",
      "(246) Watson said: -0.592795\n",
      "(247) Watson said: -0.601439\n",
      "(248) Watson said: 0.617326\n",
      "(249) Watson said: 0.537898\n",
      "(250) Watson said: -0.711745\n",
      "(251) Watson said: 0.855015\n",
      "(252) Watson said: -0.738068\n",
      "(253) Watson said: -0.534234\n",
      "(254) Watson said: -0.466667\n",
      "(255) Watson said: -0.772637\n",
      "(256) Watson said: 0\n",
      "(257) Watson said: 0.465272\n",
      "(258) Watson said: 0.546042\n",
      "(259) Watson said: -0.790696\n",
      "(260) Watson said: 0\n",
      "(261) Watson said: -0.404886\n",
      "(262) Watson said: 0.509115\n",
      "(263) Watson said: -0.578884\n",
      "(264) Watson said: -0.596774\n",
      "(265) Watson said: -0.852864\n",
      "(266) Watson said: 0\n",
      "(267) Watson said: 0.506688\n",
      "(268) Watson said: -0.80986\n",
      "(269) Watson said: 0\n",
      "(270) Watson said: 0\n",
      "(271) Watson said: 0\n",
      "(272) Watson said: 0\n",
      "(273) Watson said: 0\n",
      "(274) Watson said: -0.805332\n",
      "(275) Watson said: 0.824226\n",
      "(276) Watson said: 0.741915\n",
      "(277) Watson said: -0.941999\n",
      "(278) Watson said: 0\n",
      "(279) Watson said: -0.322938\n",
      "(280) Watson said: 0.300493\n",
      "(281) Watson said: -0.535326\n",
      "(282) Watson said: 0.74047\n",
      "(283) Watson said: 0\n",
      "(284) Watson said: 0\n",
      "(285) Watson said: -0.416052\n",
      "(286) Watson said: 0\n",
      "(287) Watson said: -0.336406\n",
      "(288) Watson said: -0.735865\n",
      "(289) Watson said: -0.450873\n",
      "(290) Watson said: -0.600439\n",
      "(291) Watson said: 0.650756\n",
      "(292) Watson said: -0.856923\n",
      "(293) Watson said: -0.799279\n",
      "(294) Watson said: 0\n",
      "(295) Watson said: 0.758609\n",
      "(296) Watson said: 0.273317\n",
      "(297) Watson said: 0.57424\n",
      "(298) Watson said: 0\n",
      "(299) Watson said: 0\n",
      "(300) Watson said: -0.288376\n",
      "(301) Watson said: 0.90639\n",
      "(302) Watson said: -0.43804\n",
      "(303) Watson said: 0.345951\n",
      "(304) Watson said: -0.442451\n",
      "(305) Watson said: 0\n",
      "(306) Watson said: 0.286104\n",
      "(307) Watson said: 0\n",
      "(308) Watson said: -0.59382\n",
      "(309) Watson said: -0.704836\n",
      "(310) Watson said: 0.372401\n",
      "(311) Watson said: 0\n",
      "(312) Watson said: -0.783876\n",
      "(313) Watson said: 0.71969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314) Watson said: -0.716789\n",
      "(315) Watson said: -0.774191\n",
      "(316) Watson said: -0.685945\n",
      "(317) Watson said: 0.307415\n",
      "(318) Watson said: 0.512047\n",
      "(319) Watson said: 0.76681\n",
      "(320) Watson said: -0.543033\n",
      "(321) Watson said: -0.32615\n",
      "(322) Watson said: -0.374754\n",
      "(323) Watson said: 0\n",
      "(324) Watson said: 0\n",
      "(325) Watson said: 0.433333\n",
      "(326) Watson said: -0.847622\n",
      "(327) Watson said: -0.475316\n",
      "(328) Watson said: 0.251176\n",
      "(329) Watson said: -0.974369\n",
      "(330) Watson said: -0.440927\n",
      "(331) Watson said: 0\n",
      "(332) Watson said: -0.753944\n",
      "(333) Watson said: -0.670537\n",
      "(334) Watson said: 0\n",
      "(335) Watson said: -0.625833\n",
      "(336) Watson said: -0.479581\n",
      "(337) Watson said: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:unsupported text language: ca\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\array\\Anaconda3\\lib\\site-packages\\ibm_cloud_sdk_core\\base_service.py\", line 225, in send\n",
      "    response.status_code, http_response=response)\n",
      "ibm_cloud_sdk_core.api_exception.ApiException: Error: unsupported text language: ca, Code: 400 , X-global-transaction-id: 3505c58fcb072500ae17abcb1c5e507a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338) Watson said: nan\n",
      "(339) Watson said: -0.787782\n",
      "(340) Watson said: -0.385569\n",
      "(341) Watson said: 0.984114\n",
      "(342) Watson said: 0.946816\n",
      "(343) Watson said: -0.920059\n",
      "(344) Watson said: 0\n",
      "(345) Watson said: 0.834984\n",
      "(346) Watson said: 0\n",
      "(347) Watson said: -0.577162\n",
      "(348) Watson said: 0.46053\n",
      "(349) Watson said: 0.758709\n",
      "(350) Watson said: 0.831549\n",
      "(351) Watson said: -0.799399\n",
      "(352) Watson said: -0.932474\n",
      "(353) Watson said: -0.853616\n",
      "(354) Watson said: -0.599355\n",
      "(355) Watson said: -0.553014\n",
      "(356) Watson said: 0\n",
      "(357) Watson said: -0.519449\n",
      "(358) Watson said: 0.426008\n",
      "(359) Watson said: -0.529973\n",
      "(360) Watson said: -0.587762\n",
      "(361) Watson said: -0.92285\n",
      "(362) Watson said: 0\n",
      "(363) Watson said: -0.715093\n",
      "(364) Watson said: -0.907211\n",
      "(365) Watson said: -0.313718\n",
      "(366) Watson said: 0\n",
      "(367) Watson said: -0.634458\n",
      "(368) Watson said: -0.815829\n",
      "(369) Watson said: 0\n",
      "(370) Watson said: -0.883665\n",
      "(371) Watson said: 0.757983\n",
      "(372) Watson said: -0.940531\n",
      "(373) Watson said: 0.517471\n",
      "(374) Watson said: -0.910921\n",
      "(375) Watson said: 0.408141\n",
      "(376) Watson said: -0.868954\n",
      "(377) Watson said: -0.837513\n",
      "(378) Watson said: -0.996518\n",
      "(379) Watson said: 0\n",
      "(380) Watson said: 0.955828\n"
     ]
    }
   ],
   "source": [
    "# Let do that iteratively\n",
    "for i in range(len(alex_df)):\n",
    "    \n",
    "    # The API requires a minimum of XX words??\n",
    "    if ( len(alex_df['Text'].iloc[i].split()) > 1 ):\n",
    "        \n",
    "        # Get the results for user i:\n",
    "        score = get_Watson_Sentiment(alex_df['Text'].iloc[i])\n",
    "        print(\"(\"+str(i)+\") Watson said: \" + str(score))\n",
    "        \n",
    "        # Update the feature vectors\n",
    "        alex_df['Watson_Sent'].iloc[i] = score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    380.000000\n",
       "mean      -0.108048\n",
       "std        0.584759\n",
       "min       -0.996518\n",
       "25%       -0.617178\n",
       "50%        0.000000\n",
       "75%        0.424713\n",
       "max        0.991864\n",
       "Name: Watson_Sent, dtype: float64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.Watson_Sent.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSP_Stems</th>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "      <th>Watson_Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSP_Stems</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.247254</td>\n",
       "      <td>0.447891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFINN_Stems_Pol</th>\n",
       "      <td>0.247254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Watson_Sent</th>\n",
       "      <td>0.447891</td>\n",
       "      <td>0.265410</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MSP_Stems  AFINN_Stems_Pol  Watson_Sent\n",
       "MSP_Stems         1.000000         0.247254     0.447891\n",
       "AFINN_Stems_Pol   0.247254         1.000000     0.265410\n",
       "Watson_Sent       0.447891         0.265410     1.000000"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['MSP_Stems','AFINN_Stems_Pol','Watson_Sent']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save fatures data in a new version of the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSP_Words', 'MSP_Stems', 'AFINN_Words_Pol', 'AFINN_Stems_Pol',\n",
       "       'AFINN_Words_Int', 'AFINN_Stems_Int', 'AFINN_Words_Max',\n",
       "       'AFINN_Stems_Max', 'Watson_Sent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 9 new features related to sentiment. \n",
    "alex_df.columns[len(alex_df.columns)-9:len(alex_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated features dataset\n",
    "Feats_6_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_features_6.csv\"\n",
    "alex_df.to_csv(Feats_6_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
