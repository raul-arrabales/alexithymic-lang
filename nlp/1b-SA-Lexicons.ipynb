{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Analysis of Alexithymic Discourse\n",
    "\n",
    "<hr>\n",
    "\n",
    "Alexithymic Language Project / raul@psicobotica.com / V2 release (sept 2020)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Sentiment Analysis Lexicons (Spanish)\n",
    "\n",
    "- Multilingual Sentiment Project (Spanish) [link](https://sites.google.com/site/datascienceslab/projects/multilingualsentiment).\n",
    "- AFINN-165 (English) [link](https://github.com/fnielsen/afinn). \n",
    "- AFINN-165-ES (my translation of AFINN-165). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Project (Spanish)\n",
    "Lists of positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_pos_words_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/Multilingualsentiment_positive_words_es.txt\"\n",
    "msp_neg_words_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/Multilingualsentiment_negative_words_es.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_text_file = open(msp_pos_words_path, \"r\")\n",
    "# neg_text_file = open(msp_neg_words_path, \"r\")\n",
    "# Load from URL instead of local disk\n",
    "pos_text_file =  urllib.request.urlopen(msp_pos_words_path)\n",
    "neg_text_file =  urllib.request.urlopen(msp_neg_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charset = pos_text_file.info().get_content_charset()\n",
    "charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format is one word per line\n",
    "pos_lines = pos_text_file.read().decode(charset)\n",
    "neg_lines = neg_text_file.read().decode(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lines = pos_lines.splitlines()\n",
    "neg_lines = neg_lines.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720 1555\n"
     ]
    }
   ],
   "source": [
    "print(\"%2d %2d\" % (len(neg_lines), len(pos_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aire', 'restos', 'canal', 'pasado', 'falta', 'problema']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_lines[34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grande', 'realizar', 'firme', 'profesional', 'similar', 'libre']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lines[34:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sets\n",
    "pos_words_set = set(pos_lines)\n",
    "neg_words_set = set(neg_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_words_set.intersection(neg_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4275"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_set = pos_words_set.union(neg_words_set)\n",
    "len(all_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_POS: 0; N_NEG: 1\n"
     ]
    }
   ],
   "source": [
    "test_phrase = \"Que asco de vida\".split()\n",
    "\n",
    "n_pos = np.sum([word in pos_words_set for word in test_phrase])\n",
    "n_neg = np.sum([word in neg_words_set for word in test_phrase])\n",
    "\n",
    "print(\"N_POS:%2d; N_NEG:%2d\" % (n_pos, n_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering stems instead of full words\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Porter stemmer\n",
    "p_stemmer = PorterStemmer() \n",
    "\n",
    "# Snowball stemmer\n",
    "s_stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "pos_stems_p = [p_stemmer.stem(word) for word in pos_lines]\n",
    "neg_stems_p = [p_stemmer.stem(word) for word in neg_lines]\n",
    "\n",
    "pos_stems_s = [s_stemmer.stem(word) for word in pos_lines]\n",
    "neg_stems_s = [s_stemmer.stem(word) for word in neg_lines]\n",
    "\n",
    "pos_stems_p_set = set(pos_stems_p)\n",
    "neg_stems_p_set = set(neg_stems_p)\n",
    "\n",
    "pos_stems_s_set = set(pos_stems_s)\n",
    "neg_stems_s_set = set(neg_stems_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_POS: 0; N_NEG: 2\n"
     ]
    }
   ],
   "source": [
    "test_phrase2 = \"Que asco de vida\".split()\n",
    "test_phrase2_s = [p_stemmer.stem(word) for word in test_phrase2]\n",
    "\n",
    "n_pos = np.sum([word in pos_stems_s_set for word in test_phrase2_s])\n",
    "n_neg = np.sum([word in neg_stems_s_set for word in test_phrase2_s])\n",
    "\n",
    "print(\"N_POS:%2d; N_NEG:%2d\" % (n_pos, n_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataframes\n",
    "pos_words_df = pd.DataFrame(list(pos_words_set), columns=['Pos'])\n",
    "neg_words_df = pd.DataFrame(list(neg_words_set), columns=['Neg'])\n",
    "pos_stems_s_df = pd.DataFrame(list(pos_stems_s_set), columns=['Pos'])\n",
    "neg_stems_s_df = pd.DataFrame(list(neg_stems_s_set), columns=['Neg'])\n",
    "pos_stems_p_df = pd.DataFrame(list(pos_stems_p_set), columns=['Pos'])\n",
    "neg_stems_p_df = pd.DataFrame(list(neg_stems_p_set), columns=['Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pos    1137\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_stems_s_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word sets as df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Pos_Words.csv\"\n",
    "neg_words_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Neg_Words.csv\"\n",
    "pos_stems_p_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Pos_StemsP.csv\"\n",
    "neg_stems_p_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Neg_StemsP.csv\"\n",
    "pos_stems_s_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Pos_StemsS.csv\"\n",
    "neg_stems_s_df_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\MSP_Neg_StemsS.csv\"\n",
    "\n",
    "pos_words_df.to_csv(pos_words_df_path, sep=';', encoding='utf-8', index=False)\n",
    "neg_words_df.to_csv(neg_words_df_path, sep=';', encoding='utf-8', index=False)\n",
    "pos_stems_p_df.to_csv(pos_stems_p_df_path, sep=';', encoding='utf-8', index=False)\n",
    "neg_stems_p_df.to_csv(neg_stems_p_df_path, sep=';', encoding='utf-8', index=False)\n",
    "pos_stems_s_df.to_csv(pos_stems_s_df_path, sep=';', encoding='utf-8', index=False)\n",
    "neg_stems_s_df.to_csv(neg_stems_s_df_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFINN-165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting AFINN sentiment analysis lexicon in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/lexicon/AFINN-en-165.txt\"\n",
    "\n",
    "AFINN_df = pd.read_csv(AFINN_path, header=None, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_df.columns = ['Word', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Score\n",
       "0    abandon     -2\n",
       "1  abandoned     -2\n",
       "2   abandons     -2\n",
       "3   abducted     -2\n",
       "4  abduction     -2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3382.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.617386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.124552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Score\n",
       "count  3382.000000\n",
       "mean     -0.617386\n",
       "std       2.124552\n",
       "min      -5.000000\n",
       "25%      -2.000000\n",
       "50%      -2.000000\n",
       "75%       2.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate FINN words into Spanish\n",
    "Using Google Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating with Google Translate API\n",
    "from googletrans import Translator \n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df = AFINN_df.copy()\n",
    "AFINN_es_df['Word_ES_Text'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celoso\n"
     ]
    }
   ],
   "source": [
    "# Apply in batches \n",
    "for i in range(3000,3382):\n",
    "    trlted = translator.translate(AFINN_df['Word'].iloc[i], src='en', dest='es').text\n",
    "    print(trlted)\n",
    "    AFINN_es_df['Word_ES_Text'].iloc[i] = trlted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AFINN_es_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All at once (problems with Google API)\n",
    "# AFINN_df['Word_ES'] = AFINN_df.apply(\n",
    "#     lambda row: translator.translate(row.Word, src='en', dest='es').text, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (they appeared due to translation)\n",
    "AFINN_es_df = AFINN_es_df.drop_duplicates(subset='Word_ES_Text', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AFINN_df) - len(AFINN_es_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN_es_df = pd.read_csv(\"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\AFINN-165-es.csv\", header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\array\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# All to lower\n",
    "AFINN_es_df['Word_ES_Text_lower'] = AFINN_es_df.Word_ES_Text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df.drop('Word_ES', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df.drop('Word_ES_Text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_df = AFINN_es_df.rename(columns={'Word_ES_Text_lower': 'Word_ES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "      <th>Word_ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandonado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestrado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Score      Word_ES\n",
       "0    abandon     -2    abandonar\n",
       "1  abandoned     -2   abandonado\n",
       "2   abandons     -2     abandona\n",
       "3   abducted     -2  secuestrado\n",
       "4  abduction     -2    secuestro"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_es_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abandonar'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stemmer.stem('abandonar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Porter stems\n",
    "AFINN_es_df['Stem_ES_P'] = AFINN_es_df.Word_ES.apply(lambda x: p_stemmer.stem(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Snowball stems\n",
    "AFINN_es_df['Stem_ES_S'] = AFINN_es_df.Word_ES.apply(lambda x: s_stemmer.stem(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "      <th>Word_ES</th>\n",
       "      <th>Stem_ES_P</th>\n",
       "      <th>Stem_ES_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>abandonar</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandonado</td>\n",
       "      <td>abandonado</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "      <td>abandona</td>\n",
       "      <td>abandona</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestrado</td>\n",
       "      <td>secuestrado</td>\n",
       "      <td>secuestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestro</td>\n",
       "      <td>secuestro</td>\n",
       "      <td>secuestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abductions</td>\n",
       "      <td>-2</td>\n",
       "      <td>secuestros</td>\n",
       "      <td>secuestro</td>\n",
       "      <td>secuestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abhor</td>\n",
       "      <td>-3</td>\n",
       "      <td>aborrecer</td>\n",
       "      <td>aborrec</td>\n",
       "      <td>aborrec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abhorred</td>\n",
       "      <td>-3</td>\n",
       "      <td>aborrecido</td>\n",
       "      <td>aborrecido</td>\n",
       "      <td>aborrec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abhorrent</td>\n",
       "      <td>-3</td>\n",
       "      <td>aborrecible</td>\n",
       "      <td>aborrec</td>\n",
       "      <td>aborrec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abhors</td>\n",
       "      <td>-3</td>\n",
       "      <td>aborrece</td>\n",
       "      <td>aborrec</td>\n",
       "      <td>aborrec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Score      Word_ES    Stem_ES_P Stem_ES_S\n",
       "0     abandon     -2    abandonar    abandonar   abandon\n",
       "1   abandoned     -2   abandonado   abandonado   abandon\n",
       "2    abandons     -2     abandona     abandona   abandon\n",
       "3    abducted     -2  secuestrado  secuestrado  secuestr\n",
       "4   abduction     -2    secuestro    secuestro  secuestr\n",
       "5  abductions     -2   secuestros    secuestro  secuestr\n",
       "6       abhor     -3    aborrecer      aborrec   aborrec\n",
       "7    abhorred     -3   aborrecido   aborrecido   aborrec\n",
       "8   abhorrent     -3  aborrecible      aborrec   aborrec\n",
       "9      abhors     -3     aborrece      aborrec   aborrec"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AFINN_es_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Spanish Traslated AFINN df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN_es_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Sentiment_Lexicons\\\\AFINN-165-es.csv\"\n",
    "AFINN_es_df.to_csv(AFINN_es_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
