{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Analysis of Alexithymic Discourse\n",
    "\n",
    "<hr>\n",
    "\n",
    "Alexithymic Language Project / raul@psicobotica.com / V2 release (sept 2020)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Lexicosemantics Analysis\n",
    "\n",
    "We review here the most frequent words used by participants, taking into account Part of Speech (PoS) and semantics associated to terms.\n",
    "\n",
    "- Three corpora considered: all, non-alexithymic, alexithymic. \n",
    "- Most frequent nouns. \n",
    "- Most frequent adjectives. \n",
    "- Most frequent verbs. \n",
    "\n",
    "<hr>\n",
    "\n",
    "[Explanation of Lexical Semantics](https://en.wikipedia.org/wiki/Lexical_semantics)\n",
    "\n",
    "[List of PoS tags (Spanish)](https://universaldependencies.org/docs/es/pos/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features dataset\n",
    "- Data is already pre-processed (1-Preprocessing). \n",
    "- Basic NLP features are already calculated (2-Features). \n",
    "- Some additional BoW features have been added (3-BoW).\n",
    "- Some additional TF/IDF features have been added (3-TFIDF).\n",
    "- N-Gram models have been generated (3-N-Grams). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "import heapq\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dataset_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/data/Prolexitim_v2_features_3.csv\"\n",
    "alex_df = pd.read_csv(feats_dataset_path, header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code', 'TAS20', 'F1', 'F2', 'F3', 'Gender', 'Age', 'Card',\n",
       "       'T_Metaphors', 'T_ToM', 'T_FP', 'T_Interpret', 'T_Desc', 'T_Confussion',\n",
       "       'Text', 'Alex_A', 'Alex_B', 'Words', 'Sentences', 'Tokens',\n",
       "       'Tokens_Stop', 'Tokens_Stem_P', 'Tokens_Stem_S', 'POS', 'NER', 'DEP',\n",
       "       'Lemmas_CNLP', 'Lemmas_Spacy', 'Chars', 'avgWL', 'avgSL', 'Pun_Count',\n",
       "       'Stop_Count', 'RawTokens', 'Title_Count', 'Upper_Count', 'PRON_Count',\n",
       "       'DET_Count', 'ADV_Count', 'VERB_Count', 'PROPN_Count', 'NOUN_Count',\n",
       "       'NUM_Count', 'PUNCT_Count', 'SYM_Count', 'SCONJ_Count', 'CCONJ_Count',\n",
       "       'INTJ_Count', 'AUX_Count', 'ADP_Count', 'ADJ_Count', 'PRON_Ratio',\n",
       "       'DET_Ratio', 'ADV_Ratio', 'VERB_Ratio', 'PROPN_Ratio', 'NOUN_Ratio',\n",
       "       'NUM_Ratio', 'PUNCT_Ratio', 'SYM_Ratio', 'SCONJ_Ratio', 'CCONJ_Ratio',\n",
       "       'INTJ_Ratio', 'AUX_Ratio', 'ADP_Ratio', 'ADJ_Ratio', 'TTR', 'HTR',\n",
       "       'BoW_PCA_1', 'BoW_PCA_2', 'BoW_PCA_3', 'TFIDF_PCA_1', 'TFIDF_PCA_2',\n",
       "       'TFIDF_PCA_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>TAS20</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Card</th>\n",
       "      <th>T_Metaphors</th>\n",
       "      <th>T_ToM</th>\n",
       "      <th>...</th>\n",
       "      <th>ADP_Ratio</th>\n",
       "      <th>ADJ_Ratio</th>\n",
       "      <th>TTR</th>\n",
       "      <th>HTR</th>\n",
       "      <th>BoW_PCA_1</th>\n",
       "      <th>BoW_PCA_2</th>\n",
       "      <th>BoW_PCA_3</th>\n",
       "      <th>TFIDF_PCA_1</th>\n",
       "      <th>TFIDF_PCA_2</th>\n",
       "      <th>TFIDF_PCA_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc39e22ca5dba59fbd97c27987878f56</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.429786</td>\n",
       "      <td>-0.056197</td>\n",
       "      <td>-0.360772</td>\n",
       "      <td>-0.114870</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.031455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc39e22ca5dba59fbd97c27987878f56</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>13HM</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.535592</td>\n",
       "      <td>0.971355</td>\n",
       "      <td>-0.133005</td>\n",
       "      <td>0.867802</td>\n",
       "      <td>0.301337</td>\n",
       "      <td>0.165452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.713317</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>-0.255988</td>\n",
       "      <td>-0.089725</td>\n",
       "      <td>0.143005</td>\n",
       "      <td>0.031664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>9VH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>-0.280320</td>\n",
       "      <td>-0.445467</td>\n",
       "      <td>0.372081</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.076310</td>\n",
       "      <td>-0.093545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>13HM</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.539096</td>\n",
       "      <td>0.998465</td>\n",
       "      <td>-0.135003</td>\n",
       "      <td>0.393093</td>\n",
       "      <td>0.108074</td>\n",
       "      <td>0.043623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Code  TAS20  F1  F2  F3  Gender  Age  Card  \\\n",
       "0  bc39e22ca5dba59fbd97c27987878f56     40  16   9  15       2   22     1   \n",
       "1  bc39e22ca5dba59fbd97c27987878f56     40  16   9  15       2   22  13HM   \n",
       "2  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22     1   \n",
       "3  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22   9VH   \n",
       "4  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22  13HM   \n",
       "\n",
       "   T_Metaphors  T_ToM  ...  ADP_Ratio  ADJ_Ratio       TTR       HTR  \\\n",
       "0            0      1  ...   0.125000   0.000000  0.562500  0.875000   \n",
       "1            0      1  ...   0.000000   0.000000  0.857143  1.000000   \n",
       "2            0      1  ...   0.103448   0.172414  0.344828  0.793103   \n",
       "3            0      1  ...   0.208333   0.083333  0.458333  0.875000   \n",
       "4            0      1  ...   0.100000   0.200000  0.900000  1.000000   \n",
       "\n",
       "  BoW_PCA_1  BoW_PCA_2  BoW_PCA_3  TFIDF_PCA_1  TFIDF_PCA_2 TFIDF_PCA_3  \n",
       "0  0.429786  -0.056197  -0.360772    -0.114870     0.168706    0.031455  \n",
       "1 -0.535592   0.971355  -0.133005     0.867802     0.301337    0.165452  \n",
       "2  0.713317  -0.012597  -0.255988    -0.089725     0.143005    0.031664  \n",
       "3 -0.280320  -0.445467   0.372081    -0.019208    -0.076310   -0.093545  \n",
       "4 -0.539096   0.998465  -0.135003     0.393093     0.108074    0.043623  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpora\n",
    "Let's get three corpora, one global, one with \"alexithymic language\" and the other with \"non-alexithymic language\". We'll need just the PoS tagging for each.\n",
    "\n",
    "We also have to take into acount the card presented to each participant, as the lexicon will be greatly influenced by that. \n",
    "\n",
    "- AllDoc will contain all documents from all participants. \n",
    "- AlexDoc will contain merged text from TAS-20 positive users. \n",
    "- NoAlexDoc will contain merged text from TAS-20 negative users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: es un niÃ±o pensando en cual es la respuesta de sus deberes porque no la sabe.\n",
      "\n",
      "PoS Tagged: [('es', 'AUX'), ('un', 'DET'), ('niÃ±o', 'NOUN'), ('pensando', 'VERB'), ('en', 'ADP'), ('cual', 'PRON'), ('es', 'AUX'), ('la', 'DET'), ('respuesta', 'NOUN'), ('de', 'ADP'), ('sus', 'DET'), ('deberes', 'NOUN'), ('porque', 'SCONJ'), ('no', 'ADV'), ('la', 'PRON'), ('sabe', 'VERB'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# The POS feature contains the PoS tagging for each document: \n",
    "print(\"Doc: \" + alex_df['Text'][0])\n",
    "print()\n",
    "print(\"PoS Tagged: \" + alex_df['POS'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDocs = alex_df[['POS','Card']]\n",
    "AlexDocs = alex_df[alex_df.Alex_A == 1][['POS','Card']]\n",
    "NoAlexDocs = alex_df[alex_df.Alex_A == 0][['POS','Card']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9VH     20\n",
       "1       20\n",
       "11      18\n",
       "13HM    17\n",
       "15HM     1\n",
       "Name: Card, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexDocs.Card.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9VH     69\n",
       "1       65\n",
       "11      65\n",
       "13HM    64\n",
       "12VN     8\n",
       "3VH      6\n",
       "7VH      6\n",
       "13V      5\n",
       "13N      5\n",
       "10       4\n",
       "18NM     3\n",
       "13VH     2\n",
       "!Â·HM     1\n",
       "9BM      1\n",
       "10N      1\n",
       "Name: Card, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoAlexDocs.Card.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd need to stick to cards 9VH, 1, 11 and 13HM, which are the ones mainly represented in both classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute most frequent words per grammatical function\n",
    "- Verbs. \n",
    "- Auxiliary verbs. \n",
    "- Nouns. \n",
    "- Proper nouns. \n",
    "- Adjectives. \n",
    "- Adverbs. \n",
    "- Subordinate conjunctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the list of specific PoS tokens in a list of POS tagged documents\n",
    "def get_PoS_SortedList(corpus, PoS_tag):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : series of lists of tuples (word, POS_tag)\n",
    "        Documents to be analyzed. \n",
    "    PoS_tag : str\n",
    "        The specific POS that we want to extract\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words_sorted: sorted list with K=word, V=frequency in the corpus\n",
    "        Sorted by frequency (inversed)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    words_dict = {}\n",
    "    \n",
    "    for PoSList in corpus:  # For each PoS Tagged doc\n",
    "        tag_list = ast.literal_eval(PoSList)    # Get the list of tuples\n",
    "        for PoStuple in tag_list: \n",
    "            word = PoStuple[0]\n",
    "            tag = PoStuple[1]\n",
    "            if ( tag == PoS_tag ): \n",
    "                if word not in words_dict.keys():\n",
    "                    words_dict[word] = 1\n",
    "                else:\n",
    "                    words_dict[word] += 1\n",
    "    \n",
    "    # Sort by frequency (higher first)\n",
    "    words_sorted = []\n",
    "    for w in sorted(words_dict, key=words_dict.get, reverse=True):\n",
    "        words_sorted.append((w, words_dict[w]))\n",
    "        \n",
    "    return words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ordered lists of interest\n",
    "\n",
    "# Nouns\n",
    "all_nouns = get_PoS_SortedList(AllDocs.POS, 'NOUN')\n",
    "alex_nouns = get_PoS_SortedList(AlexDocs.POS, 'NOUN')\n",
    "noalex_nouns = get_PoS_SortedList(NoAlexDocs.POS, 'NOUN')\n",
    "\n",
    "# Verbs\n",
    "all_verbs = get_PoS_SortedList(AllDocs.POS, 'VERB')\n",
    "alex_verbs = get_PoS_SortedList(AlexDocs.POS, 'VERB')\n",
    "noalex_verbs = get_PoS_SortedList(NoAlexDocs.POS, 'VERB')\n",
    "\n",
    "# Adjectives\n",
    "all_adjectives = get_PoS_SortedList(AllDocs.POS, 'ADJ')\n",
    "alex_adjectives = get_PoS_SortedList(AlexDocs.POS, 'ADJ')\n",
    "noalex_adjectives = get_PoS_SortedList(NoAlexDocs.POS, 'ADJ')\n",
    "\n",
    "# Subordinated conjunctions\n",
    "all_sconj = get_PoS_SortedList(AllDocs.POS, 'SCONJ')\n",
    "alex_sconj = get_PoS_SortedList(AlexDocs.POS, 'SCONJ')\n",
    "noalex_sconj = get_PoS_SortedList(NoAlexDocs.POS, 'SCONJ')\n",
    "\n",
    "# Adverbs\n",
    "all_adverbs = get_PoS_SortedList(AllDocs.POS, 'ADV')\n",
    "alex_adverbs = get_PoS_SortedList(AlexDocs.POS, 'ADV')\n",
    "noalex_adverbs = get_PoS_SortedList(NoAlexDocs.POS, 'ADV')\n",
    "\n",
    "# Auxiliary verbs\n",
    "all_aux = get_PoS_SortedList(AllDocs.POS, 'AUX')\n",
    "alex_aux = get_PoS_SortedList(AlexDocs.POS, 'AUX')\n",
    "noalex_aux = get_PoS_SortedList(NoAlexDocs.POS, 'AUX')\n",
    "\n",
    "# Proper nouns\n",
    "all_proper = get_PoS_SortedList(AllDocs.POS, 'PROPN')\n",
    "alex_proper = get_PoS_SortedList(AlexDocs.POS, 'PROPN')\n",
    "noalex_proper = get_PoS_SortedList(NoAlexDocs.POS, 'PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('explorers', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Propper nouns tagging for Spanish just didn't work (see Preprocessing notebook): \n",
    "all_proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tocar', 12),\n",
       " ('hacer', 7),\n",
       " ('tenÃ­a', 6),\n",
       " ('encontrar', 5),\n",
       " ('sabe', 5),\n",
       " ('aprender', 4),\n",
       " ('durmiendo', 4),\n",
       " ('descansando', 4),\n",
       " ('pensando', 3),\n",
       " ('ver', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_verbs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tocar', 41),\n",
       " ('querÃ­a', 21),\n",
       " ('tenÃ­a', 21),\n",
       " ('tiene', 20),\n",
       " ('hacer', 18),\n",
       " ('trabajar', 15),\n",
       " ('gustaba', 14),\n",
       " ('tener', 12),\n",
       " ('descansar', 12),\n",
       " ('tenÃ­an', 11)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noalex_verbs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there significant differences between alex and noalex groups?\n",
    "- Independently of the card being shown to the participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of most frequent words to analyze \n",
    "Top_N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in nouns usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View as dataframe: \n",
    "nouns_df = pd.DataFrame(list(zip(\n",
    "    list(map(itemgetter(0), alex_nouns[0:Top_N])),\n",
    "    list(map(itemgetter(0), noalex_nouns[0:Top_N])))), \n",
    "    columns=['AlexNouns','NoAlexNouns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlexNouns</th>\n",
       "      <th>NoAlexNouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>niÃ±o</td>\n",
       "      <td>violÃ­n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hombre</td>\n",
       "      <td>niÃ±o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>violÃ­n</td>\n",
       "      <td>dÃ­a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dÃ­a</td>\n",
       "      <td>hombre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mujer</td>\n",
       "      <td>mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>violin</td>\n",
       "      <td>padres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>casa</td>\n",
       "      <td>casa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grupo</td>\n",
       "      <td>vida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>trabajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esposa</td>\n",
       "      <td>grupo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AlexNouns NoAlexNouns\n",
       "0      niÃ±o      violÃ­n\n",
       "1    hombre        niÃ±o\n",
       "2    violÃ­n         dÃ­a\n",
       "3       dÃ­a      hombre\n",
       "4     mujer       mujer\n",
       "5    violin      padres\n",
       "6      casa        casa\n",
       "7     grupo        vida\n",
       "8   trabajo     trabajo\n",
       "9    esposa       grupo"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the top-N sets\n",
    "def print_Set_Stats(alex_list, noalex_list, top_n):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    alex_list : list \n",
    "        List of most frequent words in alexithymia group.\n",
    "    noalex_list : list \n",
    "        List of most frequent words in non-alexithymia group.\n",
    "     top_n: int\n",
    "        Number of most frequent words to analyze.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Print stats\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    alex_set = set(list(map(itemgetter(0), alex_list[0:top_n])))\n",
    "    noalex_set = set(list(map(itemgetter(0), noalex_list[0:top_n])))\n",
    "\n",
    "    union = alex_set | noalex_set\n",
    "    intersection = alex_set & noalex_set\n",
    "    difference1 = alex_set - noalex_set\n",
    "    difference2 = noalex_set - alex_set\n",
    "    notincommon = alex_set ^ noalex_set\n",
    "\n",
    "    print(\"WORDS ANALYSIS\")\n",
    "    print(\"--------------\")\n",
    "    print(\"Alex Set:\")\n",
    "    print(alex_set)\n",
    "    print()\n",
    "    print(\"NoAlex Set:\")\n",
    "    print(noalex_set)\n",
    "    print()\n",
    "    print(\"Union:\")\n",
    "    print(union)\n",
    "    print(\"--> Union size: %d (ratio %.2f)\" % (len(union),len(union)/top_n))\n",
    "    print()\n",
    "    print(\"Intersection:\")\n",
    "    print(intersection)\n",
    "    print(\"--> Intersection size: %d (ratio %.2f)\" % (len(intersection),len(intersection)/top_n))\n",
    "    print()\n",
    "    print(\"Alex - NoAlex Difference:\")\n",
    "    print(difference1)\n",
    "    print(\"--> Difference1 size: %d (ratio %.2f)\" % (len(difference1),len(difference1)/top_n))\n",
    "    print()\n",
    "    print(\"NoAlex - Alex Difference:\")\n",
    "    print(difference2)\n",
    "    print(\"--> Difference2 size: %d (ratio %.2f)\" % (len(difference2),len(difference2)/top_n))\n",
    "    print()\n",
    "    print(\"Not in common:\")\n",
    "    print(notincommon)\n",
    "    print(\"--> Symmetric diff. size: %d (ratio %.2f)\" % (len(notincommon),len(notincommon)/top_n))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'niÃ±o', 'violÃ­n', 'dÃ­a', 'trabajo', 'mujer', 'violin', 'grupo', 'esposa', 'casa', 'hombre'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'niÃ±o', 'vida', 'violÃ­n', 'dÃ­a', 'trabajo', 'mujer', 'grupo', 'casa', 'hombre', 'padres'}\n",
      "\n",
      "Union:\n",
      "{'vida', 'violÃ­n', 'mujer', 'grupo', 'esposa', 'padres', 'niÃ±o', 'dÃ­a', 'trabajo', 'violin', 'casa', 'hombre'}\n",
      "--> Union size: 12 (ratio 1.20)\n",
      "\n",
      "Intersection:\n",
      "{'niÃ±o', 'violÃ­n', 'dÃ­a', 'trabajo', 'mujer', 'grupo', 'casa', 'hombre'}\n",
      "--> Intersection size: 8 (ratio 0.80)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'violin', 'esposa'}\n",
      "--> Difference1 size: 2 (ratio 0.20)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'vida', 'padres'}\n",
      "--> Difference2 size: 2 (ratio 0.20)\n",
      "\n",
      "Not in common:\n",
      "{'vida', 'violin', 'esposa', 'padres'}\n",
      "--> Symmetric diff. size: 4 (ratio 0.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_Set_Stats(alex_nouns, noalex_nouns, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in verbs usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'durmiendo', 'descansando', 'aprender', 'encontrar', 'ver', 'hacer', 'tenÃ­a', 'sabe', 'tocar', 'pensando'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'gustaba', 'tenÃ­an', 'tiene', 'querÃ­a', 'descansar', 'tener', 'hacer', 'tenÃ­a', 'tocar', 'trabajar'}\n",
      "\n",
      "Union:\n",
      "{'descansando', 'querÃ­a', 'tener', 'tenÃ­a', 'trabajar', 'durmiendo', 'gustaba', 'tenÃ­an', 'aprender', 'encontrar', 'tiene', 'descansar', 'ver', 'hacer', 'sabe', 'tocar', 'pensando'}\n",
      "--> Union size: 17 (ratio 1.70)\n",
      "\n",
      "Intersection:\n",
      "{'tenÃ­a', 'tocar', 'hacer'}\n",
      "--> Intersection size: 3 (ratio 0.30)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'durmiendo', 'descansando', 'aprender', 'encontrar', 'ver', 'sabe', 'pensando'}\n",
      "--> Difference1 size: 7 (ratio 0.70)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'gustaba', 'tenÃ­an', 'querÃ­a', 'descansar', 'tiene', 'tener', 'trabajar'}\n",
      "--> Difference2 size: 7 (ratio 0.70)\n",
      "\n",
      "Not in common:\n",
      "{'durmiendo', 'gustaba', 'descansando', 'tenÃ­an', 'tiene', 'querÃ­a', 'descansar', 'tener', 'aprender', 'encontrar', 'ver', 'sabe', 'trabajar', 'pensando'}\n",
      "--> Symmetric diff. size: 14 (ratio 1.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_Set_Stats(alex_verbs, noalex_verbs, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in adjectives usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'hermosa', 'cansado', 'aburrido', 'gran', 'Ãºnico', 'solitario', 'bella', 'muerta', 'juntos', 'plena'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'cansado', 'solo', 'gran', 'mejor', 'nuevo', 'siguiente', 'junto', 'muerta', 'juntos', 'triste'}\n",
      "\n",
      "Union:\n",
      "{'hermosa', 'cansado', 'Ãºnico', 'siguiente', 'junto', 'muerta', 'juntos', 'triste', 'solo', 'aburrido', 'gran', 'nuevo', 'mejor', 'solitario', 'bella', 'plena'}\n",
      "--> Union size: 16 (ratio 1.60)\n",
      "\n",
      "Intersection:\n",
      "{'muerta', 'juntos', 'cansado', 'gran'}\n",
      "--> Intersection size: 4 (ratio 0.40)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'hermosa', 'aburrido', 'Ãºnico', 'solitario', 'bella', 'plena'}\n",
      "--> Difference1 size: 6 (ratio 0.60)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'solo', 'mejor', 'nuevo', 'siguiente', 'junto', 'triste'}\n",
      "--> Difference2 size: 6 (ratio 0.60)\n",
      "\n",
      "Not in common:\n",
      "{'plena', 'hermosa', 'solo', 'aburrido', 'bella', 'Ãºnico', 'mejor', 'solitario', 'nuevo', 'siguiente', 'junto', 'triste'}\n",
      "--> Symmetric diff. size: 12 (ratio 1.20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_Set_Stats(alex_adjectives, noalex_adjectives, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there significant differences between alex and noalex groups?\n",
    "- Now, considering specific cards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Card 1:\n",
    "\n",
    "<img src=\"https://psicobotica.com/prolexitim/nlp/stimuli/TAT-1.jpg\" align=\"left\" width=320> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns\n",
    "card1_alex_nouns = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '1'].POS, 'NOUN')\n",
    "card1_noalex_nouns = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '1'].POS, 'NOUN')\n",
    "\n",
    "# Verbs\n",
    "card1_alex_verbs = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '1'].POS, 'VERB')\n",
    "card1_noalex_verbs = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '1'].POS, 'VERB')\n",
    "\n",
    "# Adjectives\n",
    "card1_alex_adjectives = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '1'].POS, 'ADJ')\n",
    "card1_noalex_adjectives = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '1'].POS, 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'niÃ±o', 'abuelo', 'violÃ­n', 'violinista', 'dÃ­a', 'clase', 'violin', 'esfuerzo', 'mÃºsica', 'padres'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'padre', 'niÃ±o', 'instrumento', 'violÃ­n', 'profesor', 'dÃ­a', 'clases', 'aÃ±os', 'mÃºsica', 'padres'}\n",
      "\n",
      "Union:\n",
      "{'abuelo', 'violÃ­n', 'instrumento', 'profesor', 'aÃ±os', 'padres', 'padre', 'niÃ±o', 'violinista', 'dÃ­a', 'clase', 'violin', 'clases', 'mÃºsica', 'esfuerzo'}\n",
      "--> Union size: 15 (ratio 1.50)\n",
      "\n",
      "Intersection:\n",
      "{'niÃ±o', 'violÃ­n', 'dÃ­a', 'mÃºsica', 'padres'}\n",
      "--> Intersection size: 5 (ratio 0.50)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'abuelo', 'violinista', 'clase', 'violin', 'esfuerzo'}\n",
      "--> Difference1 size: 5 (ratio 0.50)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'padre', 'instrumento', 'profesor', 'aÃ±os', 'clases'}\n",
      "--> Difference2 size: 5 (ratio 0.50)\n",
      "\n",
      "Not in common:\n",
      "{'padre', 'instrumento', 'violinista', 'profesor', 'abuelo', 'clase', 'violin', 'clases', 'aÃ±os', 'esfuerzo'}\n",
      "--> Symmetric diff. size: 10 (ratio 1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 1 - Nouns - Alex Vs. NoAlex\n",
    "print_Set_Stats(card1_alex_nouns, card1_noalex_nouns, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'roto', 'tiene', 'aprender', 'obligaban', 'tenÃ­a', 'odiaba', 'quedÃ³', 'tocar', 'pensando', 'dijeron'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'gustaba', 'aprender', 'querÃ­a', 'estudiar', 'tiene', 'jugar', 'tenÃ­a', 'tocar', 'sabÃ­a', 'obligaban'}\n",
      "\n",
      "Union:\n",
      "{'roto', 'querÃ­a', 'tenÃ­a', 'odiaba', 'sabÃ­a', 'obligaban', 'gustaba', 'tiene', 'aprender', 'estudiar', 'jugar', 'quedÃ³', 'tocar', 'pensando', 'dijeron'}\n",
      "--> Union size: 15 (ratio 1.50)\n",
      "\n",
      "Intersection:\n",
      "{'tiene', 'aprender', 'tenÃ­a', 'tocar', 'obligaban'}\n",
      "--> Intersection size: 5 (ratio 0.50)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'roto', 'odiaba', 'quedÃ³', 'pensando', 'dijeron'}\n",
      "--> Difference1 size: 5 (ratio 0.50)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'gustaba', 'querÃ­a', 'estudiar', 'jugar', 'sabÃ­a'}\n",
      "--> Difference2 size: 5 (ratio 0.50)\n",
      "\n",
      "Not in common:\n",
      "{'gustaba', 'roto', 'querÃ­a', 'estudiar', 'jugar', 'odiaba', 'quedÃ³', 'sabÃ­a', 'pensando', 'dijeron'}\n",
      "--> Symmetric diff. size: 10 (ratio 1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 1 - Verbs - Alex Vs. NoAlex\n",
    "print_Set_Stats(card1_alex_verbs, card1_noalex_verbs, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'frustrado', 'intentanto', 'gran', 'aburrido', 'geniales', 'estresado', 'acomodada', 'mejor', 'solitario', 'distraÃ­do'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'dormido', 'frustrado', 'cansado', 'aburrido', 'gran', 'buen', 'mejor', 'harto', 'llamado', 'triste'}\n",
      "\n",
      "Union:\n",
      "{'frustrado', 'cansado', 'estresado', 'harto', 'triste', 'distraÃ­do', 'dormido', 'intentanto', 'gran', 'aburrido', 'geniales', 'acomodada', 'buen', 'mejor', 'solitario', 'llamado'}\n",
      "--> Union size: 16 (ratio 1.60)\n",
      "\n",
      "Intersection:\n",
      "{'frustrado', 'gran', 'mejor', 'aburrido'}\n",
      "--> Intersection size: 4 (ratio 0.40)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'intentanto', 'estresado', 'geniales', 'acomodada', 'solitario', 'distraÃ­do'}\n",
      "--> Difference1 size: 6 (ratio 0.60)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'dormido', 'cansado', 'buen', 'harto', 'llamado', 'triste'}\n",
      "--> Difference2 size: 6 (ratio 0.60)\n",
      "\n",
      "Not in common:\n",
      "{'dormido', 'cansado', 'intentanto', 'estresado', 'geniales', 'buen', 'acomodada', 'solitario', 'harto', 'llamado', 'triste', 'distraÃ­do'}\n",
      "--> Symmetric diff. size: 12 (ratio 1.20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 1 - Adjectives - Alex Vs. NoAlex\n",
    "print_Set_Stats(card1_alex_adjectives, card1_noalex_adjectives, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Card 9VH:\n",
    "\n",
    "<img src=\"https://psicobotica.com/prolexitim/nlp/stimuli/TAT-9VH.jpg\" align=\"left\" width=320> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns\n",
    "card9VH_alex_nouns = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '9VH'].POS, 'NOUN')\n",
    "card9VH_noalex_nouns = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '9VH'].POS, 'NOUN')\n",
    "\n",
    "# Verbs\n",
    "card9VH_alex_verbs = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '9VH'].POS, 'VERB')\n",
    "card9VH_noalex_verbs = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '9VH'].POS, 'VERB')\n",
    "\n",
    "# Adjectives\n",
    "card9VH_alex_adjectives = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '9VH'].POS, 'ADJ')\n",
    "card9VH_noalex_adjectives = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '9VH'].POS, 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'amigos', 'sol', 'dÃ­a', 'trabajo', 'siesta', 'grupo', 'jornada', 'trabajadores', 'largo', 'campo'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'amigos', 'sol', 'dÃ­a', 'trabajo', 'siesta', 'grupo', 'hombres', 'jornada', 'campo', 'descanso'}\n",
      "\n",
      "Union:\n",
      "{'amigos', 'sol', 'siesta', 'grupo', 'trabajadores', 'largo', 'hombres', 'campo', 'dÃ­a', 'trabajo', 'jornada', 'descanso'}\n",
      "--> Union size: 12 (ratio 1.20)\n",
      "\n",
      "Intersection:\n",
      "{'amigos', 'sol', 'dÃ­a', 'trabajo', 'siesta', 'grupo', 'jornada', 'campo'}\n",
      "--> Intersection size: 8 (ratio 0.80)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'trabajadores', 'largo'}\n",
      "--> Difference1 size: 2 (ratio 0.20)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'descanso', 'hombres'}\n",
      "--> Difference2 size: 2 (ratio 0.20)\n",
      "\n",
      "Not in common:\n",
      "{'hombres', 'trabajadores', 'largo', 'descanso'}\n",
      "--> Symmetric diff. size: 4 (ratio 0.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 9VH - Nouns - Alex Vs. NoAlex\n",
    "print_Set_Stats(card9VH_alex_nouns, card9VH_noalex_nouns, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'durmiendo', 'descansando', 'encuentran', 'levantando', 'descansar', 'tomando', 'ver', 'tomar', 'realizar', 'estan'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'durmiendo', 'descansando', 'descansaban', 'descansar', 'trabajando', 'dormir', 'decidieron', 'segando', 'trabajar', 'comer'}\n",
      "\n",
      "Union:\n",
      "{'descansando', 'descansaban', 'tomando', 'realizar', 'trabajar', 'comer', 'durmiendo', 'encuentran', 'levantando', 'descansar', 'trabajando', 'ver', 'dormir', 'decidieron', 'tomar', 'segando', 'estan'}\n",
      "--> Union size: 17 (ratio 1.70)\n",
      "\n",
      "Intersection:\n",
      "{'durmiendo', 'descansando', 'descansar'}\n",
      "--> Intersection size: 3 (ratio 0.30)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'encuentran', 'levantando', 'tomando', 'ver', 'tomar', 'realizar', 'estan'}\n",
      "--> Difference1 size: 7 (ratio 0.70)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'descansaban', 'trabajando', 'dormir', 'decidieron', 'segando', 'trabajar', 'comer'}\n",
      "--> Difference2 size: 7 (ratio 0.70)\n",
      "\n",
      "Not in common:\n",
      "{'encuentran', 'levantando', 'descansaban', 'tomando', 'trabajando', 'dormir', 'ver', 'decidieron', 'tomar', 'segando', 'realizar', 'trabajar', 'comer', 'estan'}\n",
      "--> Symmetric diff. size: 14 (ratio 1.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 9VH - Verbs - Alex Vs. NoAlex\n",
    "print_Set_Stats(card9VH_alex_verbs, card9VH_noalex_verbs, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'dura', 'tumbado', 'renovadas', 'consciente', 'gran', 'buen', 'Ãºnico', 'juntos', 'plena', 'desconocido'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'dura', 'siguiente', 'larga', 'gran', 'mayor', 'largo', 'cansados', 'primeros', 'juntos', 'duro'}\n",
      "\n",
      "Union:\n",
      "{'dura', 'tumbado', 'Ãºnico', 'largo', 'siguiente', 'cansados', 'juntos', 'duro', 'renovadas', 'larga', 'consciente', 'gran', 'mayor', 'buen', 'primeros', 'plena', 'desconocido'}\n",
      "--> Union size: 17 (ratio 1.70)\n",
      "\n",
      "Intersection:\n",
      "{'dura', 'juntos', 'gran'}\n",
      "--> Intersection size: 3 (ratio 0.30)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'tumbado', 'renovadas', 'consciente', 'buen', 'Ãºnico', 'plena', 'desconocido'}\n",
      "--> Difference1 size: 7 (ratio 0.70)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'larga', 'cansados', 'mayor', 'largo', 'siguiente', 'primeros', 'duro'}\n",
      "--> Difference2 size: 7 (ratio 0.70)\n",
      "\n",
      "Not in common:\n",
      "{'tumbado', 'renovadas', 'siguiente', 'larga', 'consciente', 'mayor', 'buen', 'Ãºnico', 'largo', 'cansados', 'primeros', 'duro', 'plena', 'desconocido'}\n",
      "--> Symmetric diff. size: 14 (ratio 1.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 9VH - Adjectives - Alex Vs. NoAlex\n",
    "print_Set_Stats(card9VH_alex_adjectives, card9VH_noalex_adjectives, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Card 11:\n",
    "\n",
    "<img src=\"https://psicobotica.com/prolexitim/nlp/stimuli/TAT-11.jpg\" align=\"left\" width=320> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns\n",
    "card11_alex_nouns = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '11'].POS, 'NOUN')\n",
    "card11_noalex_nouns = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '11'].POS, 'NOUN')\n",
    "\n",
    "# Verbs\n",
    "card11_alex_verbs = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '11'].POS, 'VERB')\n",
    "card11_noalex_verbs = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '11'].POS, 'VERB')\n",
    "\n",
    "# Adjectives\n",
    "card11_alex_adjectives = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '11'].POS, 'ADJ')\n",
    "card11_noalex_adjectives = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '11'].POS, 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'cascada', 'montaÃ±a', 'hombre', 'camino', 'bosque', 'lugar', 'animales', 'final', 'catarata', 'viaje'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'dragÃ³n', 'cascada', 'montaÃ±a', 'dÃ­a', 'camino', 'agua', 'bosque', 'tesoro', 'lugar', 'piedras'}\n",
      "\n",
      "Union:\n",
      "{'dragÃ³n', 'tesoro', 'animales', 'catarata', 'cascada', 'montaÃ±a', 'piedras', 'dÃ­a', 'camino', 'bosque', 'agua', 'lugar', 'hombre', 'final', 'viaje'}\n",
      "--> Union size: 15 (ratio 1.50)\n",
      "\n",
      "Intersection:\n",
      "{'montaÃ±a', 'cascada', 'camino', 'bosque', 'lugar'}\n",
      "--> Intersection size: 5 (ratio 0.50)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'viaje', 'animales', 'hombre', 'final', 'catarata'}\n",
      "--> Difference1 size: 5 (ratio 0.50)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'dragÃ³n', 'dÃ­a', 'agua', 'tesoro', 'piedras'}\n",
      "--> Difference2 size: 5 (ratio 0.50)\n",
      "\n",
      "Not in common:\n",
      "{'dragÃ³n', 'final', 'dÃ­a', 'agua', 'tesoro', 'viaje', 'animales', 'hombre', 'piedras', 'catarata'}\n",
      "--> Symmetric diff. size: 10 (ratio 1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 11 - Nouns - Alex Vs. NoAlex\n",
    "print_Set_Stats(card11_alex_nouns, card11_noalex_nouns, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'vio', 'hizo', 'saben', 'encontrar', 'encontraba', 'llegado', 'caminando', 'tenÃ­a', 'cruzar', 'habia'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'veo', 'lleva', 'tenÃ­an', 'disfrutar', 'cayÃ³', 'llegado', 'llegar', 'hacer', 'pasar', 'tenÃ­a'}\n",
      "\n",
      "Union:\n",
      "{'vio', 'hizo', 'saben', 'disfrutar', 'cayÃ³', 'caminando', 'llegar', 'tenÃ­a', 'pasar', 'habia', 'veo', 'lleva', 'tenÃ­an', 'encontrar', 'encontraba', 'llegado', 'hacer', 'cruzar'}\n",
      "--> Union size: 18 (ratio 1.80)\n",
      "\n",
      "Intersection:\n",
      "{'tenÃ­a', 'llegado'}\n",
      "--> Intersection size: 2 (ratio 0.20)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'vio', 'hizo', 'saben', 'encontraba', 'encontrar', 'caminando', 'cruzar', 'habia'}\n",
      "--> Difference1 size: 8 (ratio 0.80)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'veo', 'lleva', 'cayÃ³', 'tenÃ­an', 'disfrutar', 'llegar', 'hacer', 'pasar'}\n",
      "--> Difference2 size: 8 (ratio 0.80)\n",
      "\n",
      "Not in common:\n",
      "{'veo', 'vio', 'lleva', 'hizo', 'saben', 'tenÃ­an', 'disfrutar', 'cayÃ³', 'encontrar', 'encontraba', 'caminando', 'llegar', 'hacer', 'pasar', 'cruzar', 'habia'}\n",
      "--> Symmetric diff. size: 16 (ratio 1.60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 11 - Verbs - Alex Vs. NoAlex\n",
    "print_Set_Stats(card11_alex_verbs, card11_noalex_verbs, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'maravilloso', 'hermosa', 'fantÃ¡sticos', 'frondoso', 'gran', 'mÃ¡gico', 'originales', 'lleno', 'turbulentos', 'vistas'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'alto', 'dormido', 'escondida', 'impresionante', 'cansado', 'gran', 'debido', 'altos', 'escondido', 'precioso'}\n",
      "\n",
      "Union:\n",
      "{'maravilloso', 'hermosa', 'impresionante', 'cansado', 'frondoso', 'originales', 'debido', 'turbulentos', 'alto', 'dormido', 'fantÃ¡sticos', 'gran', 'mÃ¡gico', 'altos', 'lleno', 'vistas', 'precioso', 'escondido', 'escondida'}\n",
      "--> Union size: 19 (ratio 1.90)\n",
      "\n",
      "Intersection:\n",
      "{'gran'}\n",
      "--> Intersection size: 1 (ratio 0.10)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'maravilloso', 'hermosa', 'fantÃ¡sticos', 'frondoso', 'mÃ¡gico', 'originales', 'lleno', 'turbulentos', 'vistas'}\n",
      "--> Difference1 size: 9 (ratio 0.90)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'alto', 'dormido', 'impresionante', 'cansado', 'debido', 'altos', 'precioso', 'escondido', 'escondida'}\n",
      "--> Difference2 size: 9 (ratio 0.90)\n",
      "\n",
      "Not in common:\n",
      "{'maravilloso', 'impresionante', 'frondoso', 'originales', 'turbulentos', 'mÃ¡gico', 'lleno', 'escondida', 'precioso', 'hermosa', 'cansado', 'debido', 'alto', 'dormido', 'fantÃ¡sticos', 'altos', 'vistas', 'escondido'}\n",
      "--> Symmetric diff. size: 18 (ratio 1.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 11 - Adjectives - Alex Vs. NoAlex\n",
    "print_Set_Stats(card11_alex_adjectives, card11_noalex_adjectives, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Card 13HM:\n",
    "\n",
    "<img src=\"https://psicobotica.com/prolexitim/nlp/stimuli/TAT-13HM.jpg\" align=\"left\" width=320> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouns\n",
    "card13HM_alex_nouns = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '13HM'].POS, 'NOUN')\n",
    "card13HM_noalex_nouns = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '13HM'].POS, 'NOUN')\n",
    "\n",
    "# Verbs\n",
    "card13HM_alex_verbs = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '13HM'].POS, 'VERB')\n",
    "card13HM_noalex_verbs = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '13HM'].POS, 'VERB')\n",
    "\n",
    "# Adjectives\n",
    "card13HM_alex_adjectives = get_PoS_SortedList(AlexDocs[AlexDocs.Card == '13HM'].POS, 'ADJ')\n",
    "card13HM_noalex_adjectives = get_PoS_SortedList(NoAlexDocs[NoAlexDocs.Card == '13HM'].POS, 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'cama', 'despues', 'trabajo', 'mujer', 'amante', 'esposa', 'pÃ©rdida', 'casa', 'hombre', 'noche'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'vida', 'ojos', 'dÃ­a', 'casa', 'trabajo', 'mujer', 'pareja', 'cama', 'hombre', 'noche'}\n",
      "\n",
      "Union:\n",
      "{'vida', 'ojos', 'casa', 'amante', 'mujer', 'pareja', 'esposa', 'noche', 'dÃ­a', 'despues', 'trabajo', 'pÃ©rdida', 'cama', 'hombre'}\n",
      "--> Union size: 14 (ratio 1.40)\n",
      "\n",
      "Intersection:\n",
      "{'cama', 'trabajo', 'mujer', 'casa', 'hombre', 'noche'}\n",
      "--> Intersection size: 6 (ratio 0.60)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'pÃ©rdida', 'esposa', 'despues', 'amante'}\n",
      "--> Difference1 size: 4 (ratio 0.40)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'dÃ­a', 'vida', 'ojos', 'pareja'}\n",
      "--> Difference2 size: 4 (ratio 0.40)\n",
      "\n",
      "Not in common:\n",
      "{'vida', 'ojos', 'dÃ­a', 'amante', 'despues', 'pareja', 'esposa', 'pÃ©rdida'}\n",
      "--> Symmetric diff. size: 8 (ratio 0.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 13HM - Nouns - Alex Vs. NoAlex\n",
    "print_Set_Stats(card13HM_alex_nouns, card13HM_noalex_nouns, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'juan', 'mata', 'encontrar', 'hacer', 'tenÃ­a', 'amenazÃ³', 'sabe', 'buscar', 'asi', 'va'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'pasado', 'querÃ­a', 'tiene', 'llorando', 'ver', 'hacer', 'tenÃ­a', 'pasar', 'encontrÃ³', 'trabajar'}\n",
      "\n",
      "Union:\n",
      "{'querÃ­a', 'llorando', 'tenÃ­a', 'amenazÃ³', 'pasar', 'trabajar', 'asi', 'va', 'juan', 'mata', 'pasado', 'tiene', 'encontrar', 'ver', 'hacer', 'sabe', 'buscar', 'encontrÃ³'}\n",
      "--> Union size: 18 (ratio 1.80)\n",
      "\n",
      "Intersection:\n",
      "{'tenÃ­a', 'hacer'}\n",
      "--> Intersection size: 2 (ratio 0.20)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'juan', 'mata', 'encontrar', 'amenazÃ³', 'sabe', 'buscar', 'asi', 'va'}\n",
      "--> Difference1 size: 8 (ratio 0.80)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'pasado', 'tiene', 'querÃ­a', 'llorando', 'ver', 'pasar', 'encontrÃ³', 'trabajar'}\n",
      "--> Difference2 size: 8 (ratio 0.80)\n",
      "\n",
      "Not in common:\n",
      "{'pasado', 'juan', 'mata', 'querÃ­a', 'tiene', 'encontrar', 'llorando', 'ver', 'pasar', 'amenazÃ³', 'sabe', 'encontrÃ³', 'buscar', 'trabajar', 'asi', 'va'}\n",
      "--> Symmetric diff. size: 16 (ratio 1.60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 13HM - Verbs - Alex Vs. NoAlex\n",
    "print_Set_Stats(card13HM_alex_verbs, card13HM_noalex_verbs, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'pequeÃ±a', 'cansado', 'solo', 'incomodo', 'buen', 'bella', 'muerta', 'juntos', 'plena', 'humilde'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'pobre', 'normal', 'solo', 'desconsolado', 'agotado', 'nuevo', 'dormida', 'fallecida', 'muerta', 'triste'}\n",
      "\n",
      "Union:\n",
      "{'pequeÃ±a', 'cansado', 'desconsolado', 'incomodo', 'agotado', 'fallecida', 'muerta', 'juntos', 'normal', 'triste', 'pobre', 'solo', 'buen', 'dormida', 'nuevo', 'bella', 'plena', 'humilde'}\n",
      "--> Union size: 18 (ratio 1.80)\n",
      "\n",
      "Intersection:\n",
      "{'solo', 'muerta'}\n",
      "--> Intersection size: 2 (ratio 0.20)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'pequeÃ±a', 'cansado', 'incomodo', 'buen', 'bella', 'juntos', 'plena', 'humilde'}\n",
      "--> Difference1 size: 8 (ratio 0.80)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'pobre', 'desconsolado', 'agotado', 'dormida', 'nuevo', 'normal', 'triste', 'fallecida'}\n",
      "--> Difference2 size: 8 (ratio 0.80)\n",
      "\n",
      "Not in common:\n",
      "{'pequeÃ±a', 'plena', 'pobre', 'normal', 'cansado', 'desconsolado', 'incomodo', 'agotado', 'nuevo', 'buen', 'dormida', 'fallecida', 'bella', 'juntos', 'triste', 'humilde'}\n",
      "--> Symmetric diff. size: 16 (ratio 1.60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Card 13HM - Adjectives - Alex Vs. NoAlex\n",
    "print_Set_Stats(card13HM_alex_adjectives, card13HM_noalex_adjectives, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the list of nouns, verbs and adjectives\n",
    "- In order of appearance (for possible further analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the list of specific PoS tokens keeping the order of appearance in the doc. \n",
    "def get_PoS_List(doc, PoS_tag):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : lists of tuples (word, POS_tag) representing a PoS tagged document.\n",
    "        Documents to be analyzed. \n",
    "    PoS_tag : str\n",
    "        The specific POS that we want to extract\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words: list with specific words keeping the order of appearance. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    tag_list = ast.literal_eval(doc)    # Get the list of tuples representing the doc. \n",
    "    for PoStuple in tag_list: \n",
    "        word = PoStuple[0]\n",
    "        tag = PoStuple[1]\n",
    "        if ( tag == PoS_tag ): \n",
    "            words.append(word)    # Add to the list only the grammatical function we want\n",
    "          \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: es un niÃ±o pensando en cual es la respuesta de sus deberes porque no la sabe.\n",
      "\n",
      "Verbs: ['pensando', 'sabe']\n",
      "\n",
      "Nouns: ['niÃ±o', 'respuesta', 'deberes']\n",
      "\n",
      "Adjectives: []\n",
      "\n",
      "Sub. Conj.: ['porque']\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"Doc: \" + alex_df['Text'][0])\n",
    "print()\n",
    "print(\"Verbs: \" + str(get_PoS_List(alex_df['POS'][0],'VERB')))\n",
    "print()\n",
    "print(\"Nouns: \" + str(get_PoS_List(alex_df['POS'][0],'NOUN')))\n",
    "print()\n",
    "print(\"Adjectives: \" + str(get_PoS_List(alex_df['POS'][0],'ADJ')))\n",
    "print()\n",
    "print(\"Sub. Conj.: \" + str(get_PoS_List(alex_df['POS'][0],'SCONJ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the list of specific parts of speech\n",
    "alex_df['Verb_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'VERB'))\n",
    "alex_df['Noun_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'NOUN'))\n",
    "alex_df['Adjective_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'ADJ'))\n",
    "alex_df['Subord_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'SCONJ'))\n",
    "alex_df['Adverb_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'ADV'))\n",
    "alex_df['Aux_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'AUX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Verb_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>[('un', 'DET'), ('hombre', 'NOUN'), ('que', 'P...</td>\n",
       "      <td>[encontrÃ³]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>[('parece', 'AUX'), ('como', 'SCONJ'), ('una',...</td>\n",
       "      <td>[encontraron, enamoraron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>[('despuÃ©s', 'ADV'), ('de', 'ADP'), ('un', 'DE...</td>\n",
       "      <td>[llegÃ³, mereciÃ³, aguantar, saborear, prepara]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>[('erase', 'VERB'), ('una', 'DET'), ('vez', 'N...</td>\n",
       "      <td>[erase, encontrar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   POS  \\\n",
       "256  [('un', 'DET'), ('hombre', 'NOUN'), ('que', 'P...   \n",
       "82   [('parece', 'AUX'), ('como', 'SCONJ'), ('una',...   \n",
       "228  [('despuÃ©s', 'ADV'), ('de', 'ADP'), ('un', 'DE...   \n",
       "327  [('erase', 'VERB'), ('una', 'DET'), ('vez', 'N...   \n",
       "\n",
       "                                         Verb_List  \n",
       "256                                     [encontrÃ³]  \n",
       "82                       [encontraron, enamoraron]  \n",
       "228  [llegÃ³, mereciÃ³, aguantar, saborear, prepara]  \n",
       "327                             [erase, encontrar]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['POS','Verb_List']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Aux_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>[('es', 'AUX'), ('una', 'DET'), ('mujer', 'NOU...</td>\n",
       "      <td>[es, estÃ¡, estÃ¡, es]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>[('un', 'DET'), ('grupo', 'NOUN'), ('de', 'ADP...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>[('despuÃ©s', 'ADV'), ('de', 'ADP'), ('un', 'DE...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[('un', 'DET'), ('niÃ±o', 'NOUN'), ('que', 'PRO...</td>\n",
       "      <td>[iba, era]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   POS              Aux_List\n",
       "117  [('es', 'AUX'), ('una', 'DET'), ('mujer', 'NOU...  [es, estÃ¡, estÃ¡, es]\n",
       "192  [('un', 'DET'), ('grupo', 'NOUN'), ('de', 'ADP...                    []\n",
       "228  [('despuÃ©s', 'ADV'), ('de', 'ADP'), ('un', 'DE...                    []\n",
       "54   [('un', 'DET'), ('niÃ±o', 'NOUN'), ('que', 'PRO...            [iba, era]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['POS','Aux_List']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code', 'TAS20', 'F1', 'F2', 'F3', 'Gender', 'Age', 'Card',\n",
       "       'T_Metaphors', 'T_ToM', 'T_FP', 'T_Interpret', 'T_Desc', 'T_Confussion',\n",
       "       'Text', 'Alex_A', 'Alex_B', 'Words', 'Sentences', 'Tokens',\n",
       "       'Tokens_Stop', 'Tokens_Stem_P', 'Tokens_Stem_S', 'POS', 'NER', 'DEP',\n",
       "       'Lemmas_CNLP', 'Lemmas_Spacy', 'Chars', 'avgWL', 'avgSL', 'Pun_Count',\n",
       "       'Stop_Count', 'RawTokens', 'Title_Count', 'Upper_Count', 'PRON_Count',\n",
       "       'DET_Count', 'ADV_Count', 'VERB_Count', 'PROPN_Count', 'NOUN_Count',\n",
       "       'NUM_Count', 'PUNCT_Count', 'SYM_Count', 'SCONJ_Count', 'CCONJ_Count',\n",
       "       'INTJ_Count', 'AUX_Count', 'ADP_Count', 'ADJ_Count', 'PRON_Ratio',\n",
       "       'DET_Ratio', 'ADV_Ratio', 'VERB_Ratio', 'PROPN_Ratio', 'NOUN_Ratio',\n",
       "       'NUM_Ratio', 'PUNCT_Ratio', 'SYM_Ratio', 'SCONJ_Ratio', 'CCONJ_Ratio',\n",
       "       'INTJ_Ratio', 'AUX_Ratio', 'ADP_Ratio', 'ADJ_Ratio', 'TTR', 'HTR',\n",
       "       'BoW_PCA_1', 'BoW_PCA_2', 'BoW_PCA_3', 'TFIDF_PCA_1', 'TFIDF_PCA_2',\n",
       "       'TFIDF_PCA_3', 'Verb_List', 'Noun_List', 'Adjective_List',\n",
       "       'Subord_List', 'Adverb_List', 'Aux_List'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated features dataset\n",
    "Feats_4_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_features_4.csv\"\n",
    "alex_df.to_csv(Feats_4_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
