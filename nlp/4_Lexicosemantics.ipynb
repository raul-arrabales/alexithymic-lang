{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Analysis of Alexithymic Discourse\n",
    "\n",
    "<hr>\n",
    "\n",
    "Alexithymic Language Project / raul@psicobotica.com / V2 release (sept 2020)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Lexicosemantics Analysis\n",
    "\n",
    "We review here the most frequent words used by participants, taking into account Part of Speech (PoS) and semantics associated to terms.\n",
    "\n",
    "- Three corpora considered: all, non-alexithymic, alexithymic. \n",
    "- Most frequent nouns. \n",
    "- Most frequent adjectives. \n",
    "- Most frequent verbs. \n",
    "\n",
    "<hr>\n",
    "\n",
    "[Explanation of Lexical Semantics](https://en.wikipedia.org/wiki/Lexical_semantics)\n",
    "\n",
    "[List of PoS tags (Spanish)](https://universaldependencies.org/docs/es/pos/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features dataset\n",
    "- Data is already pre-processed (1-Preprocessing). \n",
    "- Basic NLP features are already calculated (2-Features). \n",
    "- Some additional BoW features have been added (3-BoW).\n",
    "- Some additional TF/IDF features have been added (3-TFIDF).\n",
    "- N-Gram models have been generated (3-N-Grams). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "import heapq\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dataset_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/data/Prolexitim_v2_features_3.csv\"\n",
    "alex_df = pd.read_csv(feats_dataset_path, header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code', 'TAS20', 'F1', 'F2', 'F3', 'Gender', 'Age', 'Card',\n",
       "       'T_Metaphors', 'T_ToM', 'T_FP', 'T_Interpret', 'T_Desc', 'T_Confussion',\n",
       "       'Text', 'Alex_A', 'Alex_B', 'Words', 'Sentences', 'Tokens',\n",
       "       'Tokens_Stop', 'Tokens_Stem_P', 'Tokens_Stem_S', 'POS', 'NER', 'DEP',\n",
       "       'Lemmas_CNLP', 'Lemmas_Spacy', 'Chars', 'avgWL', 'avgSL', 'Pun_Count',\n",
       "       'Stop_Count', 'RawTokens', 'Title_Count', 'Upper_Count', 'PRON_Count',\n",
       "       'DET_Count', 'ADV_Count', 'VERB_Count', 'PROPN_Count', 'NOUN_Count',\n",
       "       'NUM_Count', 'PUNCT_Count', 'SYM_Count', 'SCONJ_Count', 'CCONJ_Count',\n",
       "       'INTJ_Count', 'AUX_Count', 'ADP_Count', 'ADJ_Count', 'PRON_Ratio',\n",
       "       'DET_Ratio', 'ADV_Ratio', 'VERB_Ratio', 'PROPN_Ratio', 'NOUN_Ratio',\n",
       "       'NUM_Ratio', 'PUNCT_Ratio', 'SYM_Ratio', 'SCONJ_Ratio', 'CCONJ_Ratio',\n",
       "       'INTJ_Ratio', 'AUX_Ratio', 'ADP_Ratio', 'ADJ_Ratio', 'TTR', 'HTR',\n",
       "       'BoW_PCA_1', 'BoW_PCA_2', 'BoW_PCA_3', 'TFIDF_PCA_1', 'TFIDF_PCA_2',\n",
       "       'TFIDF_PCA_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>TAS20</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Card</th>\n",
       "      <th>T_Metaphors</th>\n",
       "      <th>T_ToM</th>\n",
       "      <th>...</th>\n",
       "      <th>ADP_Ratio</th>\n",
       "      <th>ADJ_Ratio</th>\n",
       "      <th>TTR</th>\n",
       "      <th>HTR</th>\n",
       "      <th>BoW_PCA_1</th>\n",
       "      <th>BoW_PCA_2</th>\n",
       "      <th>BoW_PCA_3</th>\n",
       "      <th>TFIDF_PCA_1</th>\n",
       "      <th>TFIDF_PCA_2</th>\n",
       "      <th>TFIDF_PCA_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc39e22ca5dba59fbd97c27987878f56</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.429786</td>\n",
       "      <td>-0.056197</td>\n",
       "      <td>-0.360772</td>\n",
       "      <td>-0.114870</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.031455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc39e22ca5dba59fbd97c27987878f56</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>13HM</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.535592</td>\n",
       "      <td>0.971355</td>\n",
       "      <td>-0.133005</td>\n",
       "      <td>0.867802</td>\n",
       "      <td>0.301337</td>\n",
       "      <td>0.165452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.713317</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>-0.255988</td>\n",
       "      <td>-0.089725</td>\n",
       "      <td>0.143005</td>\n",
       "      <td>0.031664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>9VH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>-0.280320</td>\n",
       "      <td>-0.445467</td>\n",
       "      <td>0.372081</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.076310</td>\n",
       "      <td>-0.093545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>13HM</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.539096</td>\n",
       "      <td>0.998465</td>\n",
       "      <td>-0.135003</td>\n",
       "      <td>0.393093</td>\n",
       "      <td>0.108074</td>\n",
       "      <td>0.043623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Code  TAS20  F1  F2  F3  Gender  Age  Card  \\\n",
       "0  bc39e22ca5dba59fbd97c27987878f56     40  16   9  15       2   22     1   \n",
       "1  bc39e22ca5dba59fbd97c27987878f56     40  16   9  15       2   22  13HM   \n",
       "2  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22     1   \n",
       "3  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22   9VH   \n",
       "4  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22  13HM   \n",
       "\n",
       "   T_Metaphors  T_ToM  ...  ADP_Ratio  ADJ_Ratio       TTR       HTR  \\\n",
       "0            0      1  ...   0.125000   0.000000  0.562500  0.875000   \n",
       "1            0      1  ...   0.000000   0.000000  0.857143  1.000000   \n",
       "2            0      1  ...   0.103448   0.172414  0.344828  0.793103   \n",
       "3            0      1  ...   0.208333   0.083333  0.458333  0.875000   \n",
       "4            0      1  ...   0.100000   0.200000  0.900000  1.000000   \n",
       "\n",
       "  BoW_PCA_1  BoW_PCA_2  BoW_PCA_3  TFIDF_PCA_1  TFIDF_PCA_2 TFIDF_PCA_3  \n",
       "0  0.429786  -0.056197  -0.360772    -0.114870     0.168706    0.031455  \n",
       "1 -0.535592   0.971355  -0.133005     0.867802     0.301337    0.165452  \n",
       "2  0.713317  -0.012597  -0.255988    -0.089725     0.143005    0.031664  \n",
       "3 -0.280320  -0.445467   0.372081    -0.019208    -0.076310   -0.093545  \n",
       "4 -0.539096   0.998465  -0.135003     0.393093     0.108074    0.043623  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpora\n",
    "Let's get three corpora, one global, one with \"alexithymic language\" and the other with \"non-alexithymic language\". We'll need just the PoS tagging for each\n",
    "- AllDoc will contain all documents from all participants. \n",
    "- AlexDoc will contain merged text from TAS-20 positive users. \n",
    "- NoAlexDoc will contain merged text from TAS-20 negative users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: es un niÃ±o pensando en cual es la respuesta de sus deberes porque no la sabe.\n",
      "\n",
      "PoS Tagged: [('es', 'AUX'), ('un', 'DET'), ('niÃ±o', 'NOUN'), ('pensando', 'VERB'), ('en', 'ADP'), ('cual', 'PRON'), ('es', 'AUX'), ('la', 'DET'), ('respuesta', 'NOUN'), ('de', 'ADP'), ('sus', 'DET'), ('deberes', 'NOUN'), ('porque', 'SCONJ'), ('no', 'ADV'), ('la', 'PRON'), ('sabe', 'VERB'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# The POS feature contains the PoS tagging for each document: \n",
    "print(\"Doc: \" + alex_df['Text'][0])\n",
    "print()\n",
    "print(\"PoS Tagged: \" + alex_df['POS'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDocs = alex_df['POS']\n",
    "AlexDocs = alex_df[alex_df.Alex_A == 1]['POS']\n",
    "NoAlexDocs = alex_df[alex_df.Alex_A == 0]['POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute most frequent words per grammatical function\n",
    "- Verbs. \n",
    "- Auxiliary verbs. \n",
    "- Nouns. \n",
    "- Proper nouns. \n",
    "- Adjectives. \n",
    "- Adverbs. \n",
    "- Subordinate conjunctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the list of specific PoS tokens in a list of POS tagged documents\n",
    "def get_PoS_Dict(corpus, PoS_tag):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : series of lists of tuples (word, POS_tag)\n",
    "        Documents to be analyzed. \n",
    "    PoS_tag : str\n",
    "        The specific POS that we want to extract\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words_sorted: sorted list with K=word, V=frequency in the corpus\n",
    "        Sorted by frequency (inversed)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    words_dict = {}\n",
    "    \n",
    "    for PoSList in corpus:  # For each PoS Tagged doc\n",
    "        tag_list = ast.literal_eval(PoSList)    # Get the list of tuples\n",
    "        for PoStuple in tag_list: \n",
    "            word = PoStuple[0]\n",
    "            tag = PoStuple[1]\n",
    "            if ( tag == PoS_tag ): \n",
    "                if word not in words_dict.keys():\n",
    "                    words_dict[word] = 1\n",
    "                else:\n",
    "                    words_dict[word] += 1\n",
    "    \n",
    "    # Sort by frequency (higher first)\n",
    "    words_sorted = []\n",
    "    for w in sorted(words_dict, key=words_dict.get, reverse=True):\n",
    "        words_sorted.append((w, words_dict[w]))\n",
    "        \n",
    "    return words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ordered lists of interest\n",
    "\n",
    "# Nouns\n",
    "all_nouns = get_PoS_Dict(AllDocs, 'NOUN')\n",
    "alex_nouns = get_PoS_Dict(AlexDocs, 'NOUN')\n",
    "noalex_nouns = get_PoS_Dict(NoAlexDocs, 'NOUN')\n",
    "\n",
    "# Verbs\n",
    "all_verbs = get_PoS_Dict(AllDocs, 'VERB')\n",
    "alex_verbs = get_PoS_Dict(AlexDocs, 'VERB')\n",
    "noalex_verbs = get_PoS_Dict(NoAlexDocs, 'VERB')\n",
    "\n",
    "# Adjectives\n",
    "all_adjectives = get_PoS_Dict(AllDocs, 'ADJ')\n",
    "alex_adjectives = get_PoS_Dict(AlexDocs, 'ADJ')\n",
    "noalex_adjectives = get_PoS_Dict(NoAlexDocs, 'ADJ')\n",
    "\n",
    "# Subordinated conjunctions\n",
    "all_sconj = get_PoS_Dict(AllDocs, 'SCONJ')\n",
    "alex_sconj = get_PoS_Dict(AlexDocs, 'SCONJ')\n",
    "noalex_sconj = get_PoS_Dict(NoAlexDocs, 'SCONJ')\n",
    "\n",
    "# Adverbs\n",
    "all_adverbs = get_PoS_Dict(AllDocs, 'ADV')\n",
    "alex_adverbs = get_PoS_Dict(AlexDocs, 'ADV')\n",
    "noalex_adverbs = get_PoS_Dict(NoAlexDocs, 'ADV')\n",
    "\n",
    "# Auxiliary verbs\n",
    "all_aux = get_PoS_Dict(AllDocs, 'AUX')\n",
    "alex_aux = get_PoS_Dict(AlexDocs, 'AUX')\n",
    "noalex_aux = get_PoS_Dict(NoAlexDocs, 'AUX')\n",
    "\n",
    "# Proper nouns\n",
    "all_proper = get_PoS_Dict(AllDocs, 'PROPN')\n",
    "alex_proper = get_PoS_Dict(AlexDocs, 'PROPN')\n",
    "noalex_proper = get_PoS_Dict(NoAlexDocs, 'PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the list of nouns, verbs and adjectives\n",
    "- In order of appearance (for possible further analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the list of specific PoS tokens keeping the order of appearance in the doc. \n",
    "def get_PoS_List(doc, PoS_tag):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : lists of tuples (word, POS_tag) representing a PoS tagged document.\n",
    "        Documents to be analyzed. \n",
    "    PoS_tag : str\n",
    "        The specific POS that we want to extract\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words: list with specific words keeping the order of appearance. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    tag_list = ast.literal_eval(doc)    # Get the list of tuples representing the doc. \n",
    "    for PoStuple in tag_list: \n",
    "        word = PoStuple[0]\n",
    "        tag = PoStuple[1]\n",
    "        if ( tag == PoS_tag ): \n",
    "            words.append(word)    # Add to the list only the grammatical function we want\n",
    "          \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: es un niÃ±o pensando en cual es la respuesta de sus deberes porque no la sabe.\n",
      "\n",
      "Verbs: ['pensando', 'sabe']\n",
      "\n",
      "Nouns: ['niÃ±o', 'respuesta', 'deberes']\n",
      "\n",
      "Adjectives: []\n",
      "\n",
      "Sub. Conj.: ['porque']\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"Doc: \" + alex_df['Text'][0])\n",
    "print()\n",
    "print(\"Verbs: \" + str(get_PoS_List(alex_df['POS'][0],'VERB')))\n",
    "print()\n",
    "print(\"Nouns: \" + str(get_PoS_List(alex_df['POS'][0],'NOUN')))\n",
    "print()\n",
    "print(\"Adjectives: \" + str(get_PoS_List(alex_df['POS'][0],'ADJ')))\n",
    "print()\n",
    "print(\"Sub. Conj.: \" + str(get_PoS_List(alex_df['POS'][0],'SCONJ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the list of specific parts of speech\n",
    "alex_df['Verb_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'VERB'))\n",
    "alex_df['Noun_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'NOUN'))\n",
    "alex_df['Adjective_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'ADJ'))\n",
    "alex_df['Subord_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'SCONJ'))\n",
    "alex_df['Adverb_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'ADV'))\n",
    "alex_df['Aux_List'] = alex_df.POS.apply(lambda x: get_PoS_List(x,'AUX'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Verb_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>[('descansando', 'VERB'), ('en', 'ADP'), ('la'...</td>\n",
       "      <td>[descansando]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>[('tras', 'ADP'), ('una', 'DET'), ('gran', 'AD...</td>\n",
       "      <td>[relajamos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>[('dejÃ³', 'VERB'), ('el', 'DET'), ('violÃ­n', '...</td>\n",
       "      <td>[dejÃ³, posÃ³, sentÃ­a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>[('dos', 'NUM'), ('mujeres', 'NOUN'), (',', 'P...</td>\n",
       "      <td>[cayÃ©ndose, desmayÃ¡ndose]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   POS  \\\n",
       "152  [('descansando', 'VERB'), ('en', 'ADP'), ('la'...   \n",
       "156  [('tras', 'ADP'), ('una', 'DET'), ('gran', 'AD...   \n",
       "91   [('dejÃ³', 'VERB'), ('el', 'DET'), ('violÃ­n', '...   \n",
       "102  [('dos', 'NUM'), ('mujeres', 'NOUN'), (',', 'P...   \n",
       "\n",
       "                     Verb_List  \n",
       "152              [descansando]  \n",
       "156                [relajamos]  \n",
       "91        [dejÃ³, posÃ³, sentÃ­a]  \n",
       "102  [cayÃ©ndose, desmayÃ¡ndose]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['POS','Verb_List']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Aux_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>[('una', 'DET'), ('selva', 'NOUN'), ('llena', ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[('un', 'DET'), ('niÃ±o', 'NOUN'), ('que', 'PRO...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[('estragos', 'NOUN'), ('de', 'ADP'), ('la', '...</td>\n",
       "      <td>[habÃ­a, podrÃ­a, ser]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>[('un', 'DET'), ('niÃ±o', 'NOUN'), ('al', 'ADP'...</td>\n",
       "      <td>[eran, es]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   POS              Aux_List\n",
       "249  [('una', 'DET'), ('selva', 'NOUN'), ('llena', ...                    []\n",
       "95   [('un', 'DET'), ('niÃ±o', 'NOUN'), ('que', 'PRO...                    []\n",
       "170  [('estragos', 'NOUN'), ('de', 'ADP'), ('la', '...  [habÃ­a, podrÃ­a, ser]\n",
       "356  [('un', 'DET'), ('niÃ±o', 'NOUN'), ('al', 'ADP'...            [eran, es]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df[['POS','Aux_List']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code', 'TAS20', 'F1', 'F2', 'F3', 'Gender', 'Age', 'Card',\n",
       "       'T_Metaphors', 'T_ToM', 'T_FP', 'T_Interpret', 'T_Desc', 'T_Confussion',\n",
       "       'Text', 'Alex_A', 'Alex_B', 'Words', 'Sentences', 'Tokens',\n",
       "       'Tokens_Stop', 'Tokens_Stem_P', 'Tokens_Stem_S', 'POS', 'NER', 'DEP',\n",
       "       'Lemmas_CNLP', 'Lemmas_Spacy', 'Chars', 'avgWL', 'avgSL', 'Pun_Count',\n",
       "       'Stop_Count', 'RawTokens', 'Title_Count', 'Upper_Count', 'PRON_Count',\n",
       "       'DET_Count', 'ADV_Count', 'VERB_Count', 'PROPN_Count', 'NOUN_Count',\n",
       "       'NUM_Count', 'PUNCT_Count', 'SYM_Count', 'SCONJ_Count', 'CCONJ_Count',\n",
       "       'INTJ_Count', 'AUX_Count', 'ADP_Count', 'ADJ_Count', 'PRON_Ratio',\n",
       "       'DET_Ratio', 'ADV_Ratio', 'VERB_Ratio', 'PROPN_Ratio', 'NOUN_Ratio',\n",
       "       'NUM_Ratio', 'PUNCT_Ratio', 'SYM_Ratio', 'SCONJ_Ratio', 'CCONJ_Ratio',\n",
       "       'INTJ_Ratio', 'AUX_Ratio', 'ADP_Ratio', 'ADJ_Ratio', 'TTR', 'HTR',\n",
       "       'BoW_PCA_1', 'BoW_PCA_2', 'BoW_PCA_3', 'TFIDF_PCA_1', 'TFIDF_PCA_2',\n",
       "       'TFIDF_PCA_3', 'Verb_List', 'Noun_List', 'Adjective_List',\n",
       "       'Subord_List', 'Adverb_List', 'Aux_List'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated features dataset\n",
    "Feats_4_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_features_4.csv\"\n",
    "alex_df.to_csv(Feats_4_path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there significant differences between alex and noalex groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of most frequent words to analyze \n",
    "Top_N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in nouns usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View as dataframe: \n",
    "nouns_df = pd.DataFrame(list(zip(\n",
    "    list(map(itemgetter(0), alex_nouns[0:Top_N])),\n",
    "    list(map(itemgetter(0), noalex_nouns[0:Top_N])))), \n",
    "    columns=['AlexNouns','NoAlexNouns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AlexNouns</th>\n",
       "      <th>NoAlexNouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>niÃ±o</td>\n",
       "      <td>violÃ­n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hombre</td>\n",
       "      <td>niÃ±o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>violÃ­n</td>\n",
       "      <td>dÃ­a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dÃ­a</td>\n",
       "      <td>hombre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mujer</td>\n",
       "      <td>mujer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>violin</td>\n",
       "      <td>padres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>casa</td>\n",
       "      <td>casa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grupo</td>\n",
       "      <td>vida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trabajo</td>\n",
       "      <td>trabajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esposa</td>\n",
       "      <td>grupo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AlexNouns NoAlexNouns\n",
       "0      niÃ±o      violÃ­n\n",
       "1    hombre        niÃ±o\n",
       "2    violÃ­n         dÃ­a\n",
       "3       dÃ­a      hombre\n",
       "4     mujer       mujer\n",
       "5    violin      padres\n",
       "6      casa        casa\n",
       "7     grupo        vida\n",
       "8   trabajo     trabajo\n",
       "9    esposa       grupo"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the top-N sets\n",
    "def print_Set_Stats(alex_list, noalex_list, top_n):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    alex_list : list \n",
    "        List of most frequent words in alexithymia group.\n",
    "    noalex_list : list \n",
    "        List of most frequent words in non-alexithymia group.\n",
    "     top_n: int\n",
    "        Number of most frequent words to analyze.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Print stats\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    alex_set = set(list(map(itemgetter(0), alex_list[0:top_n])))\n",
    "    noalex_set = set(list(map(itemgetter(0), noalex_list[0:top_n])))\n",
    "\n",
    "    union = alex_set | noalex_set\n",
    "    intersection = alex_set & noalex_set\n",
    "    difference1 = alex_set - noalex_set\n",
    "    difference2 = noalex_set - alex_set\n",
    "    notincommon = alex_set ^ noalex_set\n",
    "\n",
    "    print(\"WORDS ANALYSIS\")\n",
    "    print(\"--------------\")\n",
    "    print(\"Alex Set:\")\n",
    "    print(alex_set)\n",
    "    print()\n",
    "    print(\"NoAlex Set:\")\n",
    "    print(noalex_set)\n",
    "    print()\n",
    "    print(\"Union:\")\n",
    "    print(union)\n",
    "    print(\"--> Union size: %d (ratio %.2f)\" % (len(union),len(union)/top_n))\n",
    "    print()\n",
    "    print(\"Intersection:\")\n",
    "    print(intersection)\n",
    "    print(\"--> Intersection size: %d (ratio %.2f)\" % (len(intersection),len(intersection)/top_n))\n",
    "    print()\n",
    "    print(\"Alex - NoAlex Difference:\")\n",
    "    print(difference1)\n",
    "    print(\"--> Difference1 size: %d (ratio %.2f)\" % (len(difference1),len(difference1)/top_n))\n",
    "    print()\n",
    "    print(\"NoAlex - Alex Difference:\")\n",
    "    print(difference2)\n",
    "    print(\"--> Difference2 size: %d (ratio %.2f)\" % (len(difference2),len(difference2)/top_n))\n",
    "    print()\n",
    "    print(\"Not in common:\")\n",
    "    print(notincommon)\n",
    "    print(\"--> Symmetric diff. size: %d (ratio %.2f)\" % (len(notincommon),len(notincommon)/top_n))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'grupo', 'casa', 'esposa', 'dÃ­a', 'violÃ­n', 'hombre', 'niÃ±o', 'mujer', 'violin', 'trabajo'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'grupo', 'casa', 'dÃ­a', 'violÃ­n', 'padres', 'hombre', 'niÃ±o', 'mujer', 'trabajo', 'vida'}\n",
      "\n",
      "Union:\n",
      "{'dÃ­a', 'violÃ­n', 'padres', 'mujer', 'violin', 'trabajo', 'vida', 'grupo', 'casa', 'esposa', 'hombre', 'niÃ±o'}\n",
      "--> Union size: 12 (ratio 1.20)\n",
      "\n",
      "Intersection:\n",
      "{'grupo', 'casa', 'dÃ­a', 'violÃ­n', 'hombre', 'niÃ±o', 'mujer', 'trabajo'}\n",
      "--> Intersection size: 8 (ratio 0.80)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'esposa', 'violin'}\n",
      "--> Difference1 size: 2 (ratio 0.20)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'padres', 'vida'}\n",
      "--> Difference2 size: 2 (ratio 0.20)\n",
      "\n",
      "Not in common:\n",
      "{'esposa', 'padres', 'violin', 'vida'}\n",
      "--> Symmetric diff. size: 4 (ratio 0.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_Set_Stats(alex_nouns, noalex_nouns, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in verbs usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'durmiendo', 'encontrar', 'ver', 'pensando', 'aprender', 'sabe', 'descansando', 'hacer', 'tenÃ­a', 'tocar'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'gustaba', 'querÃ­a', 'trabajar', 'tenÃ­an', 'tener', 'tiene', 'hacer', 'descansar', 'tenÃ­a', 'tocar'}\n",
      "\n",
      "Union:\n",
      "{'gustaba', 'aprender', 'sabe', 'descansando', 'descansar', 'tenÃ­a', 'tener', 'durmiendo', 'encontrar', 'ver', 'querÃ­a', 'pensando', 'trabajar', 'tenÃ­an', 'tiene', 'hacer', 'tocar'}\n",
      "--> Union size: 17 (ratio 1.70)\n",
      "\n",
      "Intersection:\n",
      "{'hacer', 'tenÃ­a', 'tocar'}\n",
      "--> Intersection size: 3 (ratio 0.30)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'durmiendo', 'encontrar', 'ver', 'pensando', 'aprender', 'sabe', 'descansando'}\n",
      "--> Difference1 size: 7 (ratio 0.70)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'gustaba', 'querÃ­a', 'tenÃ­an', 'trabajar', 'tiene', 'descansar', 'tener'}\n",
      "--> Difference2 size: 7 (ratio 0.70)\n",
      "\n",
      "Not in common:\n",
      "{'durmiendo', 'encontrar', 'ver', 'gustaba', 'querÃ­a', 'pensando', 'aprender', 'sabe', 'trabajar', 'tenÃ­an', 'descansando', 'tener', 'tiene', 'descansar'}\n",
      "--> Symmetric diff. size: 14 (ratio 1.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_Set_Stats(alex_verbs, noalex_verbs, Top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in adjectives usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS ANALYSIS\n",
      "--------------\n",
      "Alex Set:\n",
      "{'bella', 'plena', 'cansado', 'muerta', 'Ãºnico', 'juntos', 'gran', 'hermosa', 'aburrido', 'solitario'}\n",
      "\n",
      "NoAlex Set:\n",
      "{'siguiente', 'cansado', 'triste', 'mejor', 'muerta', 'juntos', 'junto', 'solo', 'gran', 'nuevo'}\n",
      "\n",
      "Union:\n",
      "{'plena', 'cansado', 'triste', 'muerta', 'junto', 'gran', 'hermosa', 'solitario', 'nuevo', 'siguiente', 'bella', 'mejor', 'Ãºnico', 'juntos', 'solo', 'aburrido'}\n",
      "--> Union size: 16 (ratio 1.60)\n",
      "\n",
      "Intersection:\n",
      "{'gran', 'cansado', 'muerta', 'juntos'}\n",
      "--> Intersection size: 4 (ratio 0.40)\n",
      "\n",
      "Alex - NoAlex Difference:\n",
      "{'bella', 'plena', 'Ãºnico', 'hermosa', 'aburrido', 'solitario'}\n",
      "--> Difference1 size: 6 (ratio 0.60)\n",
      "\n",
      "NoAlex - Alex Difference:\n",
      "{'siguiente', 'triste', 'mejor', 'junto', 'solo', 'nuevo'}\n",
      "--> Difference2 size: 6 (ratio 0.60)\n",
      "\n",
      "Not in common:\n",
      "{'siguiente', 'bella', 'plena', 'triste', 'mejor', 'junto', 'solo', 'Ãºnico', 'hermosa', 'aburrido', 'solitario', 'nuevo'}\n",
      "--> Symmetric diff. size: 12 (ratio 1.20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_Set_Stats(alex_adjectives, noalex_adjectives, Top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
