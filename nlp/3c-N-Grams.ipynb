{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Analysis of Alexithymic Discourse\n",
    "\n",
    "<hr>\n",
    "\n",
    "Alexithymic Language Project / raul@psicobotica.com / V2 release (sept 2020)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### N-GRAMs Models\n",
    "\n",
    "An N-Gram is \"A contiguous sequence of N items from a given sample of text or speech\".\n",
    "\n",
    "- Char N-Grams.\n",
    "- Word N-Grams.\n",
    "- Language generation with N-Gram models. \n",
    "\n",
    "<hr>\n",
    "\n",
    "[Explanation of N-Gram models](https://en.wikipedia.org/wiki/N-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features dataset\n",
    "- Data is already pre-processed (1-Preprocessing). \n",
    "- Basic NLP features are already calculated (2-Features). \n",
    "- Some additional BoW features have been added (3-BoW).\n",
    "- Some additional TF/IDF features have been added (3-TFIDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "import heapq\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dataset_path = \"https://raw.githubusercontent.com/raul-arrabales/alexithymic-lang/master/data/Prolexitim_v2_features_3.csv\"\n",
    "alex_df = pd.read_csv(feats_dataset_path, header=0, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code', 'TAS20', 'F1', 'F2', 'F3', 'Gender', 'Age', 'Card',\n",
       "       'T_Metaphors', 'T_ToM', 'T_FP', 'T_Interpret', 'T_Desc', 'T_Confussion',\n",
       "       'Text', 'Alex_A', 'Alex_B', 'Words', 'Sentences', 'Tokens',\n",
       "       'Tokens_Stop', 'Tokens_Stem_P', 'Tokens_Stem_S', 'POS', 'NER', 'DEP',\n",
       "       'Lemmas_CNLP', 'Lemmas_Spacy', 'Chars', 'avgWL', 'avgSL', 'Pun_Count',\n",
       "       'Stop_Count', 'RawTokens', 'Title_Count', 'Upper_Count', 'PRON_Count',\n",
       "       'DET_Count', 'ADV_Count', 'VERB_Count', 'PROPN_Count', 'NOUN_Count',\n",
       "       'NUM_Count', 'PUNCT_Count', 'SYM_Count', 'SCONJ_Count', 'CCONJ_Count',\n",
       "       'INTJ_Count', 'AUX_Count', 'ADP_Count', 'ADJ_Count', 'PRON_Ratio',\n",
       "       'DET_Ratio', 'ADV_Ratio', 'VERB_Ratio', 'PROPN_Ratio', 'NOUN_Ratio',\n",
       "       'NUM_Ratio', 'PUNCT_Ratio', 'SYM_Ratio', 'SCONJ_Ratio', 'CCONJ_Ratio',\n",
       "       'INTJ_Ratio', 'AUX_Ratio', 'ADP_Ratio', 'ADJ_Ratio', 'TTR', 'HTR',\n",
       "       'BoW_PCA_1', 'BoW_PCA_2', 'BoW_PCA_3', 'TFIDF_PCA_1', 'TFIDF_PCA_2',\n",
       "       'TFIDF_PCA_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>TAS20</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Card</th>\n",
       "      <th>T_Metaphors</th>\n",
       "      <th>T_ToM</th>\n",
       "      <th>...</th>\n",
       "      <th>ADP_Ratio</th>\n",
       "      <th>ADJ_Ratio</th>\n",
       "      <th>TTR</th>\n",
       "      <th>HTR</th>\n",
       "      <th>BoW_PCA_1</th>\n",
       "      <th>BoW_PCA_2</th>\n",
       "      <th>BoW_PCA_3</th>\n",
       "      <th>TFIDF_PCA_1</th>\n",
       "      <th>TFIDF_PCA_2</th>\n",
       "      <th>TFIDF_PCA_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc39e22ca5dba59fbd97c27987878f56</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.429786</td>\n",
       "      <td>-0.056197</td>\n",
       "      <td>-0.360772</td>\n",
       "      <td>-0.114870</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.031455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc39e22ca5dba59fbd97c27987878f56</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>13HM</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.535592</td>\n",
       "      <td>0.971355</td>\n",
       "      <td>-0.133005</td>\n",
       "      <td>0.867802</td>\n",
       "      <td>0.301337</td>\n",
       "      <td>0.165452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.713317</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>-0.255988</td>\n",
       "      <td>-0.089725</td>\n",
       "      <td>0.143005</td>\n",
       "      <td>0.031664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>9VH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>-0.280320</td>\n",
       "      <td>-0.445467</td>\n",
       "      <td>0.372081</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.076310</td>\n",
       "      <td>-0.093545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20cd825cadb95a71763bad06e142c148</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>13HM</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.539096</td>\n",
       "      <td>0.998465</td>\n",
       "      <td>-0.135003</td>\n",
       "      <td>0.393093</td>\n",
       "      <td>0.108074</td>\n",
       "      <td>0.043623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Code  TAS20  F1  F2  F3  Gender  Age  Card  \\\n",
       "0  bc39e22ca5dba59fbd97c27987878f56     40  16   9  15       2   22     1   \n",
       "1  bc39e22ca5dba59fbd97c27987878f56     40  16   9  15       2   22  13HM   \n",
       "2  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22     1   \n",
       "3  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22   9VH   \n",
       "4  20cd825cadb95a71763bad06e142c148     40  12  10  18       2   22  13HM   \n",
       "\n",
       "   T_Metaphors  T_ToM  ...  ADP_Ratio  ADJ_Ratio       TTR       HTR  \\\n",
       "0            0      1  ...   0.125000   0.000000  0.562500  0.875000   \n",
       "1            0      1  ...   0.000000   0.000000  0.857143  1.000000   \n",
       "2            0      1  ...   0.103448   0.172414  0.344828  0.793103   \n",
       "3            0      1  ...   0.208333   0.083333  0.458333  0.875000   \n",
       "4            0      1  ...   0.100000   0.200000  0.900000  1.000000   \n",
       "\n",
       "  BoW_PCA_1  BoW_PCA_2  BoW_PCA_3  TFIDF_PCA_1  TFIDF_PCA_2 TFIDF_PCA_3  \n",
       "0  0.429786  -0.056197  -0.360772    -0.114870     0.168706    0.031455  \n",
       "1 -0.535592   0.971355  -0.133005     0.867802     0.301337    0.165452  \n",
       "2  0.713317  -0.012597  -0.255988    -0.089725     0.143005    0.031664  \n",
       "3 -0.280320  -0.445467   0.372081    -0.019208    -0.076310   -0.093545  \n",
       "4 -0.539096   0.998465  -0.135003     0.393093     0.108074    0.043623  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpora\n",
    "Let's get two corpora, one with \"alexithymic language\" and the other with \"non-alexithymic language\". \n",
    "- AlexDoc will contain merged text from TAS-20 positive users. \n",
    "- NoAlexDoc will contain merged text from TAS-20 negative users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the alexithymic lang together\n",
    "AlexDoc = ' '.join(alex_df[alex_df.Alex_A == 1].Text)\n",
    "\n",
    "# Get all non-alexithymic lang together\n",
    "NoAlexDoc = ' '.join(alex_df[alex_df.Alex_A == 0].Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, we have a quite unbalanced dataset: Alex: 10285; NoAlex: 63589.\n"
     ]
    }
   ],
   "source": [
    "print(\"As expected, we have a quite unbalanced dataset: Alex: %d; NoAlex: %d.\" % (len(AlexDoc),len(NoAlexDoc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All to lower case and remove punctuation\n",
    "AlexDoc = AlexDoc.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "NoAlexDoc = NoAlexDoc.translate(str.maketrans('', '', string.punctuation)).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating N-Gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the length of the n-grams\n",
    "char_ngram_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char n-gram calculation function. \n",
    "def compute_char_ngrams(doc, length):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        Document to extract n-grams from. \n",
    "    length : int\n",
    "        Length of the n-grams (2 is bigram, 3 is a trigram, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    char_ngrams = {}\n",
    "    \n",
    "    for i in range(len(doc)-length):  # For each char i in doc\n",
    "        seq = doc[i:i+length]         # Get current sequence\n",
    "        # print(seq)\n",
    "        if seq not in char_ngrams.keys():       # Create entry for new sequences\n",
    "            char_ngrams[seq] = []                  \n",
    "        char_ngrams[seq].append(doc[i+length])  # Add new char\n",
    "    \n",
    "    return char_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute char n-grams for the two corpora\n",
    "alex_char_ngrams = compute_char_ngrams(AlexDoc, char_ngram_length)\n",
    "noalex_char_ngrams = compute_char_ngrams(NoAlexDoc, char_ngram_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'u', 'u', 'u']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_char_ngrams.get(\"niÃ±o q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'd', 'd', 'd', 'd']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noalex_char_ngrams.get(\"lloran\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the char n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "charNGramAlex_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_CharNGram_Alex.json\"\n",
    "charNGramNoAlex_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_CharNGram_NoAlex.json\"\n",
    "\n",
    "with open(charNGramAlex_path, 'w', encoding='utf8') as fp:\n",
    "    json.dump(alex_char_ngrams, fp, ensure_ascii=False)\n",
    "    \n",
    "with open(charNGramNoAlex_path, 'w', encoding='utf8') as fp:\n",
    "    json.dump(noalex_char_ngrams, fp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting text with the Char N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram based char generation function. \n",
    "def predict_chars(seed, ng_model, ng_length, gen_length):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : str\n",
    "        Initial sequence of chars to generate from. \n",
    "    ng_model : dict\n",
    "        Dictionary with the char N-Gram model. \n",
    "    ng_length : int\n",
    "        Length of the n-grams (2 is bigram, 3 is a trigram, etc.)\n",
    "    gen_length : int\n",
    "        Length of the string to be generated.        \n",
    "    \"\"\"\n",
    "    if (len(seed) != ng_length):\n",
    "        raise Exception(\"Seed length shoud be equal to N-Grams length.\")\n",
    "    \n",
    "    generated = seed  # We start with the seed. \n",
    "\n",
    "    for i in range(gen_length):\n",
    "        if seed not in ng_model.keys():\n",
    "            break\n",
    "        possible_chars = ng_model[seed]\n",
    "        next_char = possible_chars[random.randrange(len(possible_chars))]\n",
    "        generated += next_char\n",
    "        seed = generated[len(generated)-ng_length:len(generated)] # New seed is now the last sequence.   \n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carlitos Ã©l tiene que todos juntos y animales nunca habÃ­an pasado la noche de pasion con sus amigos y sienten muchas ideas geniales en medio de trabajo una siesta la faena que cercanÃ­a y se durmiendo a este'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Speak in \"alexithymic language\" prediction \n",
    "predict_chars(\"carlit\", alex_char_ngrams, char_ngram_length, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carlitos con mucho mÃ¡s adecuada para siempre ha terminado un hombre pobre que ahora si quedaba cierta paz y la seÃ±ores que eso le apetecÃ­a nada tocar el violÃ­n y tirar flechas con mucha mÃ¡s cercanos el agua'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Speak in \"non-alexithymic language\" prediction \n",
    "predict_chars(\"carlit\", noalex_char_ngrams, char_ngram_length, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niÃ±o que tiene un gesto un hombre le regalo su abuelitos'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Speak in \"non-alexithymic language\" prediction \n",
    "predict_chars(\"niÃ±o q\", noalex_char_ngrams, char_ngram_length, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the length of the n-grams\n",
    "# Let's go for word trigrams\n",
    "word_ngram_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word n-gram calculation function. \n",
    "def compute_word_ngrams(doc, length):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        Document to extract n-grams from. \n",
    "    length : int\n",
    "        Length of the n-grams (2 is bigram, 3 is a trigram, etc.)\n",
    "    \"\"\"\n",
    "    ngrams = {}\n",
    "    \n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    for i in range(len(tokens)-length):\n",
    "        seq = ' '.join(tokens[i:i+length])\n",
    "        # print(seq)\n",
    "        if seq not in ngrams.keys():\n",
    "            ngrams[seq] = []\n",
    "        ngrams[seq].append(tokens[i+length])\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word n-grams for the two corpora\n",
    "alex_word_ngrams = compute_word_ngrams(AlexDoc, word_ngram_length)\n",
    "noalex_word_ngrams = compute_word_ngrams(NoAlexDoc, word_ngram_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arar']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_word_ngrams.get('largo dÃ­a de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trabajo', 'trabajo']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noalex_word_ngrams.get('largo dÃ­a de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the word n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNGramAlex_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_WordNGram_Alex.json\"\n",
    "wordNGramNoAlex_path = \"D:\\\\Dropbox-Array2001\\\\Dropbox\\\\DataSets\\\\Prolexitim-Dataset\\\\Prolexitim_v2_WordNGram_NoAlex.json\"\n",
    "\n",
    "with open(wordNGramAlex_path, 'w', encoding='utf8') as fp:\n",
    "    json.dump(alex_word_ngrams, fp, ensure_ascii=False)\n",
    "    \n",
    "with open(wordNGramNoAlex_path, 'w', encoding='utf8') as fp:\n",
    "    json.dump(noalex_word_ngrams, fp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting text with the Word N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram based word generation function. \n",
    "def predict_words(seed, ng_model, ng_length, gen_length):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : str\n",
    "        Initial sequence of words to generate from. \n",
    "    ng_model : dict\n",
    "        Dictionary with the word N-Gram model. \n",
    "    ng_length : int\n",
    "        Length of the n-grams (2 is bigram, 3 is a trigram, etc.)\n",
    "    gen_length : int\n",
    "        Length of the string to be generated (in words).        \n",
    "    \"\"\"\n",
    "    seed_words = nltk.word_tokenize(seed)\n",
    "    \n",
    "    if (len(seed_words) != ng_length):\n",
    "        raise Exception(\"Seed length shoud be equal to N-Grams length.\")\n",
    "    \n",
    "    generated = seed\n",
    "    for i in range(gen_length):\n",
    "        if seed not in ng_model.keys():\n",
    "            break\n",
    "        possible_words = ng_model[seed]\n",
    "        next_word = possible_words[random.randrange(len(possible_words))]\n",
    "        generated += ' ' + next_word\n",
    "        seq_words = nltk.word_tokenize(generated)\n",
    "        seed = ' '.join(seq_words[len(seq_words)-ng_length:len(seq_words)])\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'largo dÃ­a de arar la tierra un dia una persona estaba caminando por un bosque cercano a su hogar y ha llegado a un lugar maravilloso donde la mano del hombre nunca habÃ­a llegado la naturaleza se habia encargado de poner todo en su sitio asÃ­ todos todos los elementos interactÃºaban y cooperaban'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate alexithymic text\n",
    "predict_words('largo dÃ­a de', alex_word_ngrams, word_ngram_length, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'largo dÃ­a de trabajo llegÃ³ la plÃ¡cida siesta mereciÃ³ la pena aguantar un poco mÃ¡s y saborear ese delicioso cochinillo que con tanto cariÃ±o le habÃ­a reglado su abuelo se ponÃ­a una gran presiÃ³n sobre sus hombros mÃ¡s de la que su propio abuelo pretendÃ­a con aquel presente Â¿cÃ³mo si quiera podrÃ­a iniciarse'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate non-alexithymic text\n",
    "predict_words('largo dÃ­a de', noalex_word_ngrams, word_ngram_length, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un niÃ±o que deseaba con todas sus fuerzas tocar el violÃ­n pero sus padres le obligaban a estudiar una carrera con mÃ¡s salidas un grupo de exploradores que iban a hacer un recorrido por una gran montaÃ±a pero se les hizo de noche y tuvieron que acampar todos juntos encima del cÃ©sped una cascada en mitad de la naturaleza donde todos los animales cuando se acercaba el buen tiempo se dirigÃ­an para estar horas y horas tocando el violin estaba deseoso de poder salir a jugar un grupo de trabajadores que tras la jornada de trabajo se reencontraban con sus sueÃ±os los que'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_words('un niÃ±o que', noalex_word_ngrams, word_ngram_length, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un niÃ±o que le gustaba descubrir cosas y decidiÃ³ subir por unas escaleras que descubriÃ³ cuando sus padres fueron a un bar se adentrÃ³ en un mundo mÃ¡gico en el que lo pasamos muy bien esta era la historÃ­a de un seÃ±or anciano en el que habitaban los animales mas extraÃ±os que os podrÃ­ais imaginar pero todos ellos tenÃ­an una caracterÃ­stica comÃºn todos ellos estaban privados del sentido de la vista porque no tenÃ­an ojos esta particularidad les habÃ­a llevado a desarrollar sus otros sentidos lo que a priori nos podrÃ­a parecer un handicap tenÃ­a tambiÃ©n algunas ventajas como por ejemplo que como'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_words('un niÃ±o que', noalex_word_ngrams, word_ngram_length, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un niÃ±o que querÃ­a tocar una guitarra pero le regalaron un violÃ­n estaba tumbado con sus amigos viajando en un tren destino a lo desconocido en su viaje en elefante tenÃ­a que cruzar una gran catarata fue lento y tortuoso pero al final lo consiguiÃ³ la acababa de conocer en una discoteca y habÃ­an pasado la noche juntos y ahÃ­ estaba ella muerta por un ataque al corazÃ³n y el sin saber que hacerâ€¦ un niÃ±o al que no le gustaba la mÃºsica pero sus padres le obligaban a tocar el violin un padre y sus hijos deecansando un dia de fiesta en'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_words('un niÃ±o que', alex_word_ngrams, word_ngram_length, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
